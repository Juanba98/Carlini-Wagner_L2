{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1hP6YqNeAXL9_ydtyb9tteFg8Uchp-3iU",
      "authorship_tag": "ABX9TyMBamaOE54doIa+n9KH4iT0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juanba98/Carlini-Wagner_L2/blob/main/Carlini%26Wagner_L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORyaBeRSvbnk",
        "outputId": "83e47c8f-ac3c-4212-a29f-540c6b9f1500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision  --extra-index-url https://download.pytorch.org/whl/cu116"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "h-HPSnZ9kFRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Carlini-Wagner attack (http://arxiv.org/abs/1608.04644).\n",
        "Referential implementation:\n",
        "- https://github.com/carlini/nn_robust_attacks.git\n",
        "- https://github.com/kkew3/pytorch-cw2/blob/master/cw.py\n",
        "\"\"\"\n",
        "\n",
        "import operator as op\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision import utils\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#PATHS\n",
        "from importlib.machinery import SourceFileLoader\n",
        "from os.path import join\n",
        "paths = SourceFileLoader('paths', '/content/paths.py').load_module()\n"
      ],
      "metadata": {
        "id": "W1b0se8Oys1C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "Keh9lJKzrUXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cXVs7HAbCwkM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show(imgs, gray = False):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = transforms.functional.to_pil_image(img)\n",
        "        if(gray):\n",
        "            axs[0, i].imshow(np.asarray(img).reshape(28,28),cmap=\"gray\")\n",
        "        else:\n",
        "          axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ],
      "metadata": {
        "id": "2s9P2-LWzyWt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Device to use"
      ],
      "metadata": {
        "id": "XcOauiQvjjJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA_YRyZHotVj",
        "outputId": "76411d05-1088-4239-a7dd-67b7690af9dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2 attack\n",
        "To ensure the modifitacions yields to a valid image, we use the third method of box contraints at the paper and we define  $\\delta$ as:\n",
        "\n",
        "$$\\delta_{i} = \\frac{1}{2}(tanh(w_{i}) + 1) - x_{i} $$\n",
        "\n",
        "and we optimize over the variable $w$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "9jDZY4NAHwfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attack\n",
        "\n",
        "Given x, and choosing a target class $t$  ($t \\neq C^{*}(x)$, beeing $C^{*}(x)$) the correct label of x we search for a $w$ that solves\n",
        "\n",
        "\n",
        "\n",
        "$$minimize \\ ||\\delta||^{2}_{2} + c \\cdot f(x + \\delta)$$ \n",
        "\n",
        "\n",
        "\n",
        "with $f$ defined as\n",
        "\n",
        "$$f(x') = max(max\\{Z(x')_i :  i \\neq t\\} - Z(x')_t, -\\kappa)$$\n",
        "\n",
        "To control the confidence with which missclasifation ocurrs we adjust Îº but we wil set it to 0 for out attack"
      ],
      "metadata": {
        "id": "DRSkw5svE70y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L2 formula\n",
        "\n",
        "$$||x||_{2} = \\sqrt{\\sum_{i=1}^{n} x^{2}_{i}} $$"
      ],
      "metadata": {
        "id": "cqyGiHIYFY_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "UwR6jWQ5AY6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEARCH_STEPS = 9         # number of times to adjust the constant with binary search\n",
        "MAX_ITERATIONS = 10000   # number of iterations to perform gradient descent\n",
        "ABORT_EARLY = True       # if we stop improving, abort gradient descent early\n",
        "LEARNING_RATE = 1e-2     # larger values converge faster to less accurate results\n",
        "TARGETED = True          # should we target one specific class? or just be wrong?\n",
        "K = 0                    # (kappa) how strong the adversarial example should be (confidence)\n",
        "INITIAL_CONST = 1e-3     # the initial constant c to pick as a first guess"
      ],
      "metadata": {
        "id": "_WaGTOKoAYZO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class L2Adversary(object):\n",
        "  \n",
        " \n",
        "  def __init__(self, targeted=True, k=K, c_range=(INITIAL_CONST, 1e10),\n",
        "                  search_steps=SEARCH_STEPS, max_steps=1000, abort_early=ABORT_EARLY,\n",
        "                  box=(-1., 1.), learning_rate = LEARNING_RATE):\n",
        "\n",
        "\n",
        "  \n",
        "    self.targeted = targeted # type: bool param: True for targeted attack\n",
        "    self.k = float(k) #type: int param: confidence\n",
        "    self.c_range = (float(c_range[0]), float(c_range[1])) # type: Tuple[float, float], param: range of the constat c\n",
        "    self.binary_search_steps = search_steps #type: int param: number of steps to perform binary search to find optimal c\n",
        "    self.max_steps = max_steps #type: int param: Maximum number of iterations\n",
        "    self.abort_early = abort_early #type bool param: If true, we break when the gradient descent gets stuck\n",
        "    self.box = tuple(map(float, box))  # type: Tuple[float, float], param: (max,min) pixel value\n",
        "    self.learning_rate = learning_rate #type: float param: the learning rate for Adam optimizer used over the perturbation\n",
        "    self.ae_tol = 1e-4  # tolerance of early abort\n",
        "    \n",
        "    #?\n",
        "    self.repeat = (self.binary_search_steps >= 10)\n",
        "\n",
        "    #Used for the affine transformation fo the change-of-variable\n",
        "    self.box_mul = (self.box[1]-self.box[0])/2\n",
        "    self.box_plus = (self.box[1]+self.box[0])/2\n",
        "\n",
        "  def attack(self,model, inputs, targets, num_classes ):\n",
        "\n",
        "\n",
        "    '''\n",
        "    :param model: Model to attack :type: nn.Module  \n",
        "    :param inputs: original images of dimension [B x C x H x W].\n",
        "    :param targets: the original image labels, or the attack target, of\n",
        "              dimension [B]. If ``self.targeted`` is ``True``, then ``targets``\n",
        "              is treated as the attack targets, otherwise the labels.\n",
        "              :type: ???????????????????????\n",
        "    :param num_clases: number of clases of the model :type: int  \n",
        "    :return: Adversarial examples of dimension [B X C x H x W]\n",
        "    '''\n",
        "\n",
        "\n",
        "    # sanity check\n",
        "    assert isinstance(model, nn.Module)\n",
        "    assert len(inputs.size()) == 4\n",
        "    assert len(targets.size()) == 1\n",
        "\n",
        "\n",
        "    # get a copy of targets in numpy before moving to GPU, used when doing\n",
        "    # the binary search on `scale_const`\n",
        "    targets_np = targets.clone().cpu().numpy().astype(int)  # type: np.ndarray\n",
        "\n",
        "    # we move the tensor to the same device as the model \n",
        "    inputs = inputs.to(device)  # type: torch.FloatTensor\n",
        "    targets = targets.to(device)  # type: torch.FloatTensor \n",
        "\n",
        "\n",
        "    batch_size = targets.size()[0]  # type: int\n",
        " \n",
        "    # `lower_bounds_np`, `upper_bounds_np` and `scale_consts_np` are used\n",
        "    # for binary search of each `scale_const` in the batch. The element-wise\n",
        "    # inquality holds: lower_bounds_np < scale_consts_np <= upper_bounds_np\n",
        "    lower_bounds_np = np.zeros(batch_size)\n",
        "    upper_bounds_np = np.ones(batch_size) * self.c_range[1]\n",
        "    scale_consts_np = np.ones(batch_size) * self.c_range[0]\n",
        "\n",
        "   \n",
        "\n",
        "    # Optimal attack to be found.\n",
        "    # The three \"placeholders\" are defined as:\n",
        "    # - `o_best_l2`: the least L2 norms\n",
        "    # - `o_best_l2_ppred`: the perturbed predictions made by the adversarial\n",
        "    #    perturbations with the least L2 norms\n",
        "    # - `o_best_advx`: the underlying adversarial example of\n",
        "    #   `o_best_l2_ppred`\n",
        "    o_best_l2 = np.ones(batch_size) * np.inf #dimension [B]\n",
        "    o_best_l2_ppred = -np.ones(batch_size) #dimension [B]\n",
        "    o_best_advx = inputs.clone().cpu().numpy()  # type: np.ndarray dimension [B x C x H x W]\n",
        "    \n",
        "    # convert `inputs` to tanh-space\n",
        "    #https://pytorch.org/docs/stable/generated/torch.atanh.html\n",
        "    inputs_tanh = torch.atanh((inputs - self.box_plus) / self.box_mul)\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    # the one-hot encoding of `targets`\n",
        "    targets_oh = torch.zeros(targets.size() + (num_classes,) ,requires_grad = False)  # type: torch.FloatTensor\n",
        "\n",
        "\n",
        "    #targets.unsqueeze(1) type:torch.FloatTensor but scatter need a LongTensor as a index \n",
        "    targets_oh.scatter_(1, targets.unsqueeze(1).type('torch.LongTensor'), 1.0)\n",
        "\n",
        "    targets_oh = targets_oh.to(device)\n",
        "    \n",
        "\n",
        "   \n",
        "    #the perturbation in tanh-space\n",
        "    pert_tanh = nn.Parameter(torch.zeros_like(inputs))  # type: torch.FloatTensor\n",
        "\n",
        "    \n",
        "    optimizer = optim.Adam([pert_tanh], lr=self.learning_rate)\n",
        "    \n",
        "\n",
        "    for outer_step  in range(self.binary_search_steps):\n",
        "\n",
        "      #?\n",
        "      if self.repeat and outer_step == self.binary_search_steps - 1:\n",
        "        scale_consts_np = upper_bounds_np\n",
        "\n",
        "      \n",
        "      #We convert the np.array to a tensor\n",
        "      scale_consts = torch.tensor(np.copy(scale_consts_np)).float()  # type: torch.FloatTensor\n",
        "      scale_consts = scale_consts.to(device)\n",
        "      print('Using scale consts:', list(scale_consts_np))\n",
        "\n",
        "     \n",
        "      # the minimum L2 norms of perturbations found during optimization\n",
        "      best_l2 = np.ones(batch_size) * np.inf\n",
        "\n",
        "      # the perturbed predictions corresponding to `best_l2`, to be used\n",
        "      # in binary search of `scale_const`\n",
        "      best_l2_ppred = -np.ones(batch_size)\n",
        "\n",
        "      # previous (summed) batch loss, to be used in early stopping policy\n",
        "      prev_batch_loss = np.inf  # type: float\n",
        "\n",
        "      for optim_step in range(self.max_steps):\n",
        "\n",
        "        batch_loss, pert_norms_np, pert_outputs_np, advxs_np = \\\n",
        "            self._optimize(model, optimizer, inputs_tanh,\n",
        "                            pert_tanh,  targets_oh,\n",
        "                            scale_consts)\n",
        "            \n",
        "        if optim_step % 10 == 0: print(f'batch {optim_step} loss: {batch_loss}')  \n",
        "\n",
        "        if self.abort_early and not optim_step % (self.max_steps // 10):\n",
        "\n",
        "            #If loss increse enough we abort\n",
        "            if batch_loss > prev_batch_loss * (1 - self.ae_tol):\n",
        "                break\n",
        "            prev_batch_loss = batch_loss\n",
        "\n",
        "        # update best attack found during optimization\n",
        "        pert_predictions_np = np.argmax(pert_outputs_np, axis=1)\n",
        "\n",
        "      \n",
        "        comp_pert_predictions_np = np.argmax(\n",
        "                self._compensate_confidence(pert_outputs_np,\n",
        "                                            targets_np),\n",
        "                axis=1)\n",
        "       \n",
        "        #for each image\n",
        "        for i in range(batch_size):\n",
        "\n",
        "            l2 = pert_norms_np[i]\n",
        "            cppred = comp_pert_predictions_np[i]\n",
        "            ppred = pert_predictions_np[i]\n",
        "            tlabel = targets_np[i]\n",
        "            ax = advxs_np[i]\n",
        "            if self._attack_successful(cppred, tlabel):\n",
        "                assert cppred == ppred\n",
        "                if l2 < best_l2[i]:\n",
        "                    best_l2[i] = l2\n",
        "                    best_l2_ppred[i] = ppred\n",
        "                if l2 < o_best_l2[i]:\n",
        "                    o_best_l2[i] = l2\n",
        "                    o_best_l2_ppred[i] = ppred\n",
        "                    o_best_advx[i] = ax\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # binary search of `scale_const`\n",
        "      for i in range(batch_size):\n",
        "          tlabel = targets_np[i]\n",
        "          assert best_l2_ppred[i] == -1 or \\\n",
        "                  self._attack_successful(best_l2_ppred[i], tlabel)\n",
        "          assert o_best_l2_ppred[i] == -1 or \\\n",
        "                  self._attack_successful(o_best_l2_ppred[i], tlabel)\n",
        "          if best_l2_ppred[i] != -1:\n",
        "              # successful; attempt to lower `scale_const` by halving it\n",
        "              if scale_consts_np[i] < upper_bounds_np[i]:\n",
        "                  upper_bounds_np[i] = scale_consts_np[i]\n",
        "              # `upper_bounds_np[i] == c_range[1]` implies no solution found\n",
        "             \n",
        "              if upper_bounds_np[i] < self.c_range[1] * 0.1:\n",
        "                  scale_consts_np[i] = (lower_bounds_np[i] + upper_bounds_np[i]) / 2\n",
        "          else:\n",
        "              # failure; multiply `scale_const` by ten if no solution\n",
        "              # found; otherwise do binary search\n",
        "              if scale_consts_np[i] > lower_bounds_np[i]:\n",
        "                  lower_bounds_np[i] = scale_consts_np[i]\n",
        "              if upper_bounds_np[i] < self.c_range[1] * 0.1:\n",
        "                  scale_consts_np[i] = (lower_bounds_np[i] + upper_bounds_np[i]) / 2\n",
        "              else:\n",
        "                  scale_consts_np[i] *= 10\n",
        "\n",
        "    #if not to_numpy:\n",
        "    o_best_advx = torch.from_numpy(o_best_advx).float()\n",
        "    return o_best_advx \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def _optimize(self, model, optimizer, inputs_tanh_var, pert_tanh_var,\n",
        "                  targets_oh_var, c_var):\n",
        "        \"\"\"\n",
        "        Optimize for one step.\n",
        "        :param model: the model to attack\n",
        "        :type model: nn.Module\n",
        "        :param optimizer: the Adam optimizer to optimize pert_tanh\n",
        "        :type optimizer: optim.Adam\n",
        "        :param inputs_tanh_var: the input images in tanh-space\n",
        "        :type inputs_tanh_var: torch.FloatTensor\n",
        "        :param pert_tanh_var: the perturbation to optimize in tanh-space\n",
        "        :type pert_tanh_var: torch.FloatTensor\n",
        "        :param targets_oh_var: the one-hot encoded target tensor (the attack\n",
        "               targets if self.targeted else image labels)\n",
        "        :type targets_oh_var: torch.FloatTensor\n",
        "        :param c_var: the constant `c` for each perturbation of a batch,\n",
        "               a Variable of FloatTensor of dimension [B]\n",
        "        :type c_var: torch.FloatTensor\n",
        "        :return: the batch loss, squared L2-norm of adversarial perturbations\n",
        "                 (of dimension [B]), the perturbed activations (of dimension\n",
        "                 [B]), the adversarial examples (of dimension [B x C x H x W])\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # the adversarial examples in the image space\n",
        "        # of dimension [B x C x H x W]\n",
        "        advxs_var = torch.tanh(inputs_tanh_var + pert_tanh_var) * self.box_mul + self.box_plus \n",
        "        advxs_var.to(device)\n",
        "       \n",
        "\n",
        "        # Do optimization for one step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # the perturbed activation before softmax\n",
        "        pert_outputs_var = model(advxs_var)  \n",
        "        \n",
        "\n",
        "        # the original inputs\n",
        "        inputs_var = torch.tanh(inputs_tanh_var) * self.box_mul + self.box_plus  # type: torch.FloatTensor\n",
        "        \n",
        "\n",
        "\n",
        "        #L2 norm Squared dimension [B]\n",
        "        perts_norm_var = torch.pow(advxs_var - inputs_var, 2)\n",
        "        perts_norm_var = torch.sum(perts_norm_var.view(\n",
        "                perts_norm_var.size(0), -1), 1)\n",
        "        \n",
        "      \n",
        "\n",
        "        \n",
        "        #Z(t)\n",
        "        # Z() the output of all layers except the softmax\n",
        "        # t attack target or the image label\n",
        "        target_activ_var = torch.sum(targets_oh_var * pert_outputs_var, 1)\n",
        "        inf = 1e4  \n",
        "     \n",
        "        assert (pert_outputs_var.max(1)[0] >= -inf).all(), 'assumption failed'\n",
        "       \n",
        "\n",
        "        # compute the probability of the label class versus the maximum other\n",
        "        # max(Z(x')_i : i not equal t)\n",
        "        maxother_activ_var = torch.max(((1 - targets_oh_var) * pert_outputs_var\n",
        "                                        - targets_oh_var * inf), 1)[0]\n",
        "\n",
        "        # Compute f(x'), where x' is the adversarial example in image space.\n",
        "        # The result f_var should be of dimension [B]\n",
        "        if self.targeted:\n",
        "            # if targeted, optimize to make `target_activ_var` larger than\n",
        "            # maxother_activ_var by self.k\n",
        "            f_var = torch.clamp(maxother_activ_var - target_activ_var\n",
        "                                + self.k, min=0.0)\n",
        "        else:\n",
        "            # if not targeted, optimize to make maxother_activ_var larger than\n",
        "            # target_activ_var (the ground truth image labels) by\n",
        "            # self.k\n",
        "            f_var = torch.clamp(target_activ_var - maxother_activ_var\n",
        "                                + self.k, min=0.0)\n",
        "            \n",
        "\n",
        "        # the total loss of current batch, should be of dimension [1]\n",
        "        loss = torch.sum(perts_norm_var + c_var * f_var)\n",
        "        \n",
        "\n",
        "        #Back propagation\n",
        "        loss.backward()\n",
        "        #Updates the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        \n",
        "        # Make some records in python/numpy on CPU\n",
        "        batch_loss = loss.data  # type: float\n",
        "        pert_norms_np = perts_norm_var.data.cpu().numpy()\n",
        "        pert_outputs_np = pert_outputs_var.data.cpu().numpy()\n",
        "        advxs_np = advxs_var.data.cpu().numpy()\n",
        "        return batch_loss, pert_norms_np, pert_outputs_np, advxs_np\n",
        "\n",
        "  # noinspection PyUnresolvedReferences\n",
        "  def _compensate_confidence(self, outputs, targets):\n",
        "      \"\"\"\n",
        "      Compensate for self.k and returns a new weighted sum\n",
        "      vector.\n",
        "      :param outputs: the weighted sum right before the last layer softmax\n",
        "              normalization, of dimension [B x M]\n",
        "      :type outputs: np.ndarray\n",
        "      :param targets: either the attack targets or the real image labels,\n",
        "              depending on whether or not ``self.targeted``, of dimension [B]\n",
        "      :type targets: np.ndarray\n",
        "      :return: the compensated weighted sum of dimension [B x M]\n",
        "      :rtype: np.ndarray\n",
        "      \"\"\"\n",
        "      outputs_comp = np.copy(outputs)\n",
        "      rng = np.arange(targets.shape[0])\n",
        "\n",
        "      if self.targeted:\n",
        "          # for each image i:\n",
        "          # if targeted, outputs[i, target_onehot] (target) should be larger than\n",
        "          # max(outputs[i, ~target_onehot]) (max (~target)) by self.k\n",
        "          # The target label should be larger than\n",
        "          \n",
        "          outputs_comp[rng, targets] -= self.k\n",
        "      else:\n",
        "          # for each image $i$:\n",
        "          # if not targeted, `max(outputs[i, ~target_onehot]` should be larger\n",
        "          # than `outputs[i, target_onehot]` (the ground truth image labels)\n",
        "          # by `self.k`\n",
        "          outputs_comp[rng, targets] += self.k\n",
        "\n",
        "\n",
        "      return outputs_comp\n",
        "\n",
        "\n",
        "  def _attack_successful(self, prediction, target):\n",
        "    \"\"\"\n",
        "    See whether the underlying attack is successful.\n",
        "    :param prediction: the prediction of the model on an input\n",
        "    :type prediction: int\n",
        "    :param target: either the attack target or the ground-truth image label\n",
        "    :type target: int\n",
        "    :return: ``True`` if the attack is successful\n",
        "    :rtype: bool\n",
        "    \"\"\"\n",
        "   \n",
        "    if self.targeted:\n",
        "        return prediction == target\n",
        "    else:\n",
        "        return prediction != target\n",
        "\n"
      ],
      "metadata": {
        "id": "9TH9qzesy7Ci"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CifarCNN"
      ],
      "metadata": {
        "id": "6A1JNkw0n-Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "tjEQna0jk3xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mean = [0.5,0.5,0.5]\n",
        "std =  [0.5,0.5,0.5]\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(datasets.CIFAR10(root = '/content/CIFAR10', train=True, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(mean = mean, std = std),\n",
        "                           ])),  shuffle=True, batch_size = batch_size, num_workers = 2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "NUM_CLASSES = len(classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIC1YXdOk3RW",
        "outputId": "d0909109-0ced-48a2-b098-094fa9e55d26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "F9JSIIzmkkPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CifarCNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CifarCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.fc5 = nn.Linear(512, 256)\n",
        "        self.fc6 = nn.Linear(256, 256)\n",
        "        self.fc7 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        h = F.max_pool2d(h, 4)\n",
        "\n",
        "        h = F.relu(self.bn3(self.conv3(h)))\n",
        "        h = F.relu(self.bn4(self.conv4(h)))\n",
        "        h = F.max_pool2d(h, 4)\n",
        "\n",
        "        h = F.relu(self.fc5(h.view(h.size(0), -1)))\n",
        "        h = F.relu(self.fc6(h))\n",
        "        h = self.fc7(h)\n",
        "        return F.log_softmax(h)\n",
        "\n",
        "model = CifarCNN()"
      ],
      "metadata": {
        "id": "4_YBtM_LoCBJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "RzckF-nQsyJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzkY0nfds39s",
        "outputId": "e67ef96e-b735-41d7-c1df-108ec0a7808d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 1.786\n",
            "[1,  4000] loss: 1.421\n",
            "[1,  6000] loss: 1.269\n",
            "[1,  8000] loss: 1.150\n",
            "[1, 10000] loss: 1.065\n",
            "[1, 12000] loss: 1.010\n",
            "[2,  2000] loss: 0.924\n",
            "[2,  4000] loss: 0.908\n",
            "[2,  6000] loss: 0.883\n",
            "[2,  8000] loss: 0.876\n",
            "[2, 10000] loss: 0.826\n",
            "[2, 12000] loss: 0.818\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = paths.CIFAR10_DIR\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "XoQsimMJucQe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attack"
      ],
      "metadata": {
        "id": "F9BrDfujuuMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = paths.CIFAR10_DIR\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "inputs_box = (min((0 - m) / s for m, s in zip(mean, std)),\n",
        "              max((1 - m) / s for m, s in zip(mean, std)))\n",
        "\n",
        "# a targeted adversary\n",
        "adversary = L2Adversary(targeted=True,\n",
        "                           k=0.0,\n",
        "                           search_steps=10,\n",
        "                           box=inputs_box,\n",
        "                           learning_rate=5e-4)\n",
        "\n",
        "\n",
        "inputs, labels = next(iter(dataloader)) #inputs images\n",
        "inputs.to(device)\n",
        "target_class_idx = 3\n",
        "attack_targets = torch.ones(inputs.size(0)) * target_class_idx #target one-hot encoded\n",
        "\n",
        "\n",
        "adversarial_examples = adversary.attack(model, inputs, attack_targets, NUM_CLASSES)\n",
        "assert isinstance(adversarial_examples, torch.FloatTensor)\n",
        "assert adversarial_examples.size() == inputs.size()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH6v1SF6kR5c",
        "outputId": "7756a1ea-f434-4462-af7f-715d4b5ac754"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using scale consts: [0.001, 0.001, 0.001, 0.001]\n",
            "batch 0 loss: 0.0225228164345026\n",
            "batch 10 loss: 0.0228008721023798\n",
            "batch 20 loss: 0.02256263792514801\n",
            "batch 30 loss: 0.022470513358712196\n",
            "batch 40 loss: 0.02245943248271942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 50 loss: 0.02245626226067543\n",
            "batch 60 loss: 0.022454699501395226\n",
            "batch 70 loss: 0.022454051300883293\n",
            "batch 80 loss: 0.022453851997852325\n",
            "batch 90 loss: 0.022453758865594864\n",
            "batch 100 loss: 0.022453706711530685\n",
            "batch 110 loss: 0.02245369553565979\n",
            "batch 120 loss: 0.022453684359788895\n",
            "batch 130 loss: 0.022453680634498596\n",
            "batch 140 loss: 0.022453684359788895\n",
            "batch 150 loss: 0.022453682497143745\n",
            "batch 160 loss: 0.022453684359788895\n",
            "batch 170 loss: 0.022453684359788895\n",
            "batch 180 loss: 0.022453682497143745\n",
            "batch 190 loss: 0.022453682497143745\n",
            "batch 200 loss: 0.022453684359788895\n",
            "Using scale consts: [0.01, 0.01, 0.01, 0.01]\n",
            "batch 0 loss: 0.22391356527805328\n",
            "batch 10 loss: 0.21927782893180847\n",
            "batch 20 loss: 0.2187417596578598\n",
            "batch 30 loss: 0.2185419797897339\n",
            "batch 40 loss: 0.21847543120384216\n",
            "batch 50 loss: 0.21845006942749023\n",
            "batch 60 loss: 0.21844308078289032\n",
            "batch 70 loss: 0.21844035387039185\n",
            "batch 80 loss: 0.21843843162059784\n",
            "batch 90 loss: 0.21843832731246948\n",
            "batch 100 loss: 0.21843789517879486\n",
            "batch 110 loss: 0.21843716502189636\n",
            "batch 120 loss: 0.21843823790550232\n",
            "batch 130 loss: 0.21843788027763367\n",
            "batch 140 loss: 0.21843799948692322\n",
            "batch 150 loss: 0.2184385359287262\n",
            "batch 160 loss: 0.21843908727169037\n",
            "batch 170 loss: 0.2184373140335083\n",
            "batch 180 loss: 0.21843823790550232\n",
            "batch 190 loss: 0.21843823790550232\n",
            "batch 200 loss: 0.21843771636486053\n",
            "Using scale consts: [0.1, 0.1, 0.1, 0.1]\n",
            "batch 0 loss: 2.1230080127716064\n",
            "batch 10 loss: 1.7245776653289795\n",
            "batch 20 loss: 1.6481924057006836\n",
            "batch 30 loss: 1.630246639251709\n",
            "batch 40 loss: 1.6167042255401611\n",
            "batch 50 loss: 1.6108441352844238\n",
            "batch 60 loss: 1.6099872589111328\n",
            "batch 70 loss: 1.6103084087371826\n",
            "batch 80 loss: 1.6096502542495728\n",
            "batch 90 loss: 1.6083626747131348\n",
            "batch 100 loss: 1.6085410118103027\n",
            "batch 110 loss: 1.6088271141052246\n",
            "batch 120 loss: 1.6089462041854858\n",
            "batch 130 loss: 1.6075376272201538\n",
            "batch 140 loss: 1.6097145080566406\n",
            "batch 150 loss: 1.607508659362793\n",
            "batch 160 loss: 1.6085245609283447\n",
            "batch 170 loss: 1.6080608367919922\n",
            "batch 180 loss: 1.6091289520263672\n",
            "batch 190 loss: 1.6083745956420898\n",
            "batch 200 loss: 1.6076626777648926\n",
            "batch 210 loss: 1.60771906375885\n",
            "batch 220 loss: 1.6088876724243164\n",
            "batch 230 loss: 1.610640287399292\n",
            "batch 240 loss: 1.6072689294815063\n",
            "batch 250 loss: 1.6068358421325684\n",
            "batch 260 loss: 1.6073083877563477\n",
            "batch 270 loss: 1.607566237449646\n",
            "batch 280 loss: 1.6069252490997314\n",
            "batch 290 loss: 1.6072711944580078\n",
            "batch 300 loss: 1.609015703201294\n",
            "Using scale consts: [1.0, 1.0, 1.0, 1.0]\n",
            "batch 0 loss: 11.925088882446289\n",
            "batch 10 loss: 6.098385810852051\n",
            "batch 20 loss: 4.792934417724609\n",
            "batch 30 loss: 3.5771093368530273\n",
            "batch 40 loss: 3.253105878829956\n",
            "batch 50 loss: 3.044722080230713\n",
            "batch 60 loss: 2.9416184425354004\n",
            "batch 70 loss: 2.892317771911621\n",
            "batch 80 loss: 2.7974438667297363\n",
            "batch 90 loss: 2.7808823585510254\n",
            "batch 100 loss: 2.7890138626098633\n",
            "batch 110 loss: 2.7458834648132324\n",
            "batch 120 loss: 2.735107421875\n",
            "batch 130 loss: 2.7336487770080566\n",
            "batch 140 loss: 2.7408792972564697\n",
            "batch 150 loss: 2.7387914657592773\n",
            "batch 160 loss: 2.7275571823120117\n",
            "batch 170 loss: 2.706911087036133\n",
            "batch 180 loss: 2.7240567207336426\n",
            "batch 190 loss: 2.7157392501831055\n",
            "batch 200 loss: 2.7261886596679688\n",
            "batch 210 loss: 2.715470314025879\n",
            "batch 220 loss: 2.721615791320801\n",
            "batch 230 loss: 2.711012363433838\n",
            "batch 240 loss: 2.711322784423828\n",
            "batch 250 loss: 2.6941161155700684\n",
            "batch 260 loss: 2.703118085861206\n",
            "batch 270 loss: 2.7014055252075195\n",
            "batch 280 loss: 2.6988465785980225\n",
            "batch 290 loss: 2.7081446647644043\n",
            "batch 300 loss: 2.728388547897339\n",
            "Using scale consts: [0.55, 0.55, 0.55, 0.55]\n",
            "batch 0 loss: 2.7136683464050293\n",
            "batch 10 loss: 2.692239761352539\n",
            "batch 20 loss: 2.6795785427093506\n",
            "batch 30 loss: 2.6806693077087402\n",
            "batch 40 loss: 2.6860523223876953\n",
            "batch 50 loss: 2.679865598678589\n",
            "batch 60 loss: 2.679570436477661\n",
            "batch 70 loss: 2.6789653301239014\n",
            "batch 80 loss: 2.6724140644073486\n",
            "batch 90 loss: 2.6791610717773438\n",
            "batch 100 loss: 2.6805169582366943\n",
            "batch 110 loss: 2.6775741577148438\n",
            "batch 120 loss: 2.6829545497894287\n",
            "batch 130 loss: 2.6723990440368652\n",
            "batch 140 loss: 2.674544334411621\n",
            "batch 150 loss: 2.6824917793273926\n",
            "batch 160 loss: 2.6753993034362793\n",
            "batch 170 loss: 2.675994873046875\n",
            "batch 180 loss: 2.6772031784057617\n",
            "batch 190 loss: 2.6760506629943848\n",
            "batch 200 loss: 2.6809849739074707\n",
            "Using scale consts: [0.325, 0.325, 0.325, 0.325]\n",
            "batch 0 loss: 2.673217296600342\n",
            "batch 10 loss: 2.65606689453125\n",
            "batch 20 loss: 2.6403908729553223\n",
            "batch 30 loss: 2.6300530433654785\n",
            "batch 40 loss: 2.629054546356201\n",
            "batch 50 loss: 2.627483367919922\n",
            "batch 60 loss: 2.625962495803833\n",
            "batch 70 loss: 2.6208744049072266\n",
            "batch 80 loss: 2.6176962852478027\n",
            "batch 90 loss: 2.6208765506744385\n",
            "batch 100 loss: 2.6175832748413086\n",
            "batch 110 loss: 2.6131763458251953\n",
            "batch 120 loss: 2.6091690063476562\n",
            "batch 130 loss: 2.609830856323242\n",
            "batch 140 loss: 2.6042582988739014\n",
            "batch 150 loss: 2.602518081665039\n",
            "batch 160 loss: 2.6008810997009277\n",
            "batch 170 loss: 2.600531816482544\n",
            "batch 180 loss: 2.6001486778259277\n",
            "batch 190 loss: 2.599155902862549\n",
            "batch 200 loss: 2.5993871688842773\n",
            "batch 210 loss: 2.598236083984375\n",
            "batch 220 loss: 2.5982606410980225\n",
            "batch 230 loss: 2.596799612045288\n",
            "batch 240 loss: 2.5974881649017334\n",
            "batch 250 loss: 2.597708225250244\n",
            "batch 260 loss: 2.597001552581787\n",
            "batch 270 loss: 2.598402261734009\n",
            "batch 280 loss: 2.598018169403076\n",
            "batch 290 loss: 2.5977442264556885\n",
            "batch 300 loss: 2.594970703125\n",
            "batch 310 loss: 2.598602771759033\n",
            "batch 320 loss: 2.598249912261963\n",
            "batch 330 loss: 2.594434976577759\n",
            "batch 340 loss: 2.5986437797546387\n",
            "batch 350 loss: 2.595132350921631\n",
            "batch 360 loss: 2.5952024459838867\n",
            "batch 370 loss: 2.593698740005493\n",
            "batch 380 loss: 2.595534324645996\n",
            "batch 390 loss: 2.594912528991699\n",
            "batch 400 loss: 2.600409746170044\n",
            "Using scale consts: [0.4375, 0.4375, 0.21250000000000002, 0.21250000000000002]\n",
            "batch 0 loss: 2.6883811950683594\n",
            "batch 10 loss: 2.6657509803771973\n",
            "batch 20 loss: 2.6541459560394287\n",
            "batch 30 loss: 2.646693468093872\n",
            "batch 40 loss: 2.639941692352295\n",
            "batch 50 loss: 2.643972873687744\n",
            "batch 60 loss: 2.640669345855713\n",
            "batch 70 loss: 2.639765739440918\n",
            "batch 80 loss: 2.6338553428649902\n",
            "batch 90 loss: 2.641573905944824\n",
            "batch 100 loss: 2.6314568519592285\n",
            "batch 110 loss: 2.6349167823791504\n",
            "batch 120 loss: 2.630605697631836\n",
            "batch 130 loss: 2.630190372467041\n",
            "batch 140 loss: 2.6267948150634766\n",
            "batch 150 loss: 2.627697467803955\n",
            "batch 160 loss: 2.628307819366455\n",
            "batch 170 loss: 2.6296396255493164\n",
            "batch 180 loss: 2.6290836334228516\n",
            "batch 190 loss: 2.6279332637786865\n",
            "batch 200 loss: 2.630033016204834\n",
            "batch 210 loss: 2.6313517093658447\n",
            "batch 220 loss: 2.6329336166381836\n",
            "batch 230 loss: 2.631295680999756\n",
            "batch 240 loss: 2.627506732940674\n",
            "batch 250 loss: 2.6285572052001953\n",
            "batch 260 loss: 2.6301441192626953\n",
            "batch 270 loss: 2.62791109085083\n",
            "batch 280 loss: 2.6310763359069824\n",
            "batch 290 loss: 2.626603126525879\n",
            "batch 300 loss: 2.627018928527832\n",
            "batch 310 loss: 2.6286237239837646\n",
            "batch 320 loss: 2.6321160793304443\n",
            "batch 330 loss: 2.626363515853882\n",
            "batch 340 loss: 2.628098487854004\n",
            "batch 350 loss: 2.6283621788024902\n",
            "batch 360 loss: 2.6324257850646973\n",
            "batch 370 loss: 2.627553939819336\n",
            "batch 380 loss: 2.627995491027832\n",
            "batch 390 loss: 2.6296024322509766\n",
            "batch 400 loss: 2.626150608062744\n",
            "batch 410 loss: 2.630807638168335\n",
            "batch 420 loss: 2.6294727325439453\n",
            "batch 430 loss: 2.629324436187744\n",
            "batch 440 loss: 2.637244701385498\n",
            "batch 450 loss: 2.6329727172851562\n",
            "batch 460 loss: 2.6293110847473145\n",
            "batch 470 loss: 2.633225917816162\n",
            "batch 480 loss: 2.631427526473999\n",
            "batch 490 loss: 2.6278133392333984\n",
            "batch 500 loss: 2.637969493865967\n",
            "Using scale consts: [0.49375, 0.49375, 0.26875000000000004, 0.26875000000000004]\n",
            "batch 0 loss: 2.697990894317627\n",
            "batch 10 loss: 2.6859049797058105\n",
            "batch 20 loss: 2.6794700622558594\n",
            "batch 30 loss: 2.6792101860046387\n",
            "batch 40 loss: 2.675227642059326\n",
            "batch 50 loss: 2.673720598220825\n",
            "batch 60 loss: 2.671311855316162\n",
            "batch 70 loss: 2.6758522987365723\n",
            "batch 80 loss: 2.672605514526367\n",
            "batch 90 loss: 2.6674880981445312\n",
            "batch 100 loss: 2.665255546569824\n",
            "batch 110 loss: 2.671077251434326\n",
            "batch 120 loss: 2.6727757453918457\n",
            "batch 130 loss: 2.666853427886963\n",
            "batch 140 loss: 2.6703453063964844\n",
            "batch 150 loss: 2.665140390396118\n",
            "batch 160 loss: 2.6612789630889893\n",
            "batch 170 loss: 2.659024238586426\n",
            "batch 180 loss: 2.6600968837738037\n",
            "batch 190 loss: 2.6621100902557373\n",
            "batch 200 loss: 2.663723945617676\n",
            "batch 210 loss: 2.6583542823791504\n",
            "batch 220 loss: 2.6573123931884766\n",
            "batch 230 loss: 2.656719207763672\n",
            "batch 240 loss: 2.6627650260925293\n",
            "batch 250 loss: 2.6603338718414307\n",
            "batch 260 loss: 2.663074254989624\n",
            "batch 270 loss: 2.656330108642578\n",
            "batch 280 loss: 2.6557133197784424\n",
            "batch 290 loss: 2.6578831672668457\n",
            "batch 300 loss: 2.661579132080078\n",
            "batch 310 loss: 2.6572318077087402\n",
            "batch 320 loss: 2.6553940773010254\n",
            "batch 330 loss: 2.663628339767456\n",
            "batch 340 loss: 2.6621580123901367\n",
            "batch 350 loss: 2.658562660217285\n",
            "batch 360 loss: 2.6589717864990234\n",
            "batch 370 loss: 2.656585216522217\n",
            "batch 380 loss: 2.661388874053955\n",
            "batch 390 loss: 2.6597962379455566\n",
            "batch 400 loss: 2.6564555168151855\n",
            "batch 410 loss: 2.6559395790100098\n",
            "batch 420 loss: 2.662957191467285\n",
            "batch 430 loss: 2.6586506366729736\n",
            "batch 440 loss: 2.6601099967956543\n",
            "batch 450 loss: 2.655839204788208\n",
            "batch 460 loss: 2.659879684448242\n",
            "batch 470 loss: 2.661855459213257\n",
            "batch 480 loss: 2.6600823402404785\n",
            "batch 490 loss: 2.656130313873291\n",
            "batch 500 loss: 2.6558752059936523\n",
            "batch 510 loss: 2.6537623405456543\n",
            "batch 520 loss: 2.6554477214813232\n",
            "batch 530 loss: 2.654749631881714\n",
            "batch 540 loss: 2.654482841491699\n",
            "batch 550 loss: 2.657947063446045\n",
            "batch 560 loss: 2.6555469036102295\n",
            "batch 570 loss: 2.6535415649414062\n",
            "batch 580 loss: 2.6526660919189453\n",
            "batch 590 loss: 2.659257173538208\n",
            "batch 600 loss: 2.657721757888794\n",
            "Using scale consts: [0.5218750000000001, 0.465625, 0.296875, 0.296875]\n",
            "batch 0 loss: 2.6629433631896973\n",
            "batch 10 loss: 2.661773443222046\n",
            "batch 20 loss: 2.6638240814208984\n",
            "batch 30 loss: 2.6622090339660645\n",
            "batch 40 loss: 2.6685047149658203\n",
            "batch 50 loss: 2.6612212657928467\n",
            "batch 60 loss: 2.664447069168091\n",
            "batch 70 loss: 2.657609462738037\n",
            "batch 80 loss: 2.6653923988342285\n",
            "batch 90 loss: 2.6602022647857666\n",
            "batch 100 loss: 2.661538600921631\n",
            "batch 110 loss: 2.661440849304199\n",
            "batch 120 loss: 2.6584622859954834\n",
            "batch 130 loss: 2.660583734512329\n",
            "batch 140 loss: 2.6611742973327637\n",
            "batch 150 loss: 2.6613173484802246\n",
            "batch 160 loss: 2.6616106033325195\n",
            "batch 170 loss: 2.661189556121826\n",
            "batch 180 loss: 2.6600356101989746\n",
            "batch 190 loss: 2.6581854820251465\n",
            "batch 200 loss: 2.659454822540283\n",
            "batch 210 loss: 2.6574196815490723\n",
            "batch 220 loss: 2.6558079719543457\n",
            "batch 230 loss: 2.6618881225585938\n",
            "batch 240 loss: 2.6601476669311523\n",
            "batch 250 loss: 2.6649341583251953\n",
            "batch 260 loss: 2.6704235076904297\n",
            "batch 270 loss: 2.6597771644592285\n",
            "batch 280 loss: 2.661058187484741\n",
            "batch 290 loss: 2.6629791259765625\n",
            "batch 300 loss: 2.658301830291748\n",
            "batch 310 loss: 2.664393424987793\n",
            "batch 320 loss: 2.6605124473571777\n",
            "batch 330 loss: 2.6656179428100586\n",
            "batch 340 loss: 2.665310859680176\n",
            "batch 350 loss: 2.6617140769958496\n",
            "batch 360 loss: 2.6576757431030273\n",
            "batch 370 loss: 2.6606521606445312\n",
            "batch 380 loss: 2.6614813804626465\n",
            "batch 390 loss: 2.66109561920166\n",
            "batch 400 loss: 2.664388656616211\n",
            "Using scale consts: [0.55, 0.465625, 0.296875, 0.296875]\n",
            "batch 0 loss: 2.6643471717834473\n",
            "batch 10 loss: 2.664241313934326\n",
            "batch 20 loss: 2.6662697792053223\n",
            "batch 30 loss: 2.6568825244903564\n",
            "batch 40 loss: 2.657623529434204\n",
            "batch 50 loss: 2.6600911617279053\n",
            "batch 60 loss: 2.6622915267944336\n",
            "batch 70 loss: 2.6589529514312744\n",
            "batch 80 loss: 2.668445110321045\n",
            "batch 90 loss: 2.6619982719421387\n",
            "batch 100 loss: 2.6631715297698975\n",
            "batch 110 loss: 2.659090995788574\n",
            "batch 120 loss: 2.6633903980255127\n",
            "batch 130 loss: 2.6653716564178467\n",
            "batch 140 loss: 2.6620383262634277\n",
            "batch 150 loss: 2.662907600402832\n",
            "batch 160 loss: 2.659295082092285\n",
            "batch 170 loss: 2.661595344543457\n",
            "batch 180 loss: 2.6629045009613037\n",
            "batch 190 loss: 2.660160779953003\n",
            "batch 200 loss: 2.6597037315368652\n",
            "batch 210 loss: 2.6603169441223145\n",
            "batch 220 loss: 2.6621460914611816\n",
            "batch 230 loss: 2.664008378982544\n",
            "batch 240 loss: 2.662947177886963\n",
            "batch 250 loss: 2.6573245525360107\n",
            "batch 260 loss: 2.660813331604004\n",
            "batch 270 loss: 2.658146619796753\n",
            "batch 280 loss: 2.6576766967773438\n",
            "batch 290 loss: 2.6622676849365234\n",
            "batch 300 loss: 2.6596508026123047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Results"
      ],
      "metadata": {
        "id": "wNOwO27tuxH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision as TV\n",
        "inv_normalize = transforms.Normalize(\n",
        "        mean=mean,\n",
        "        std=std\n",
        "      )\n",
        "\n",
        "\n",
        "for i in range(0,adversarial_examples.size()[0]):\n",
        " \n",
        "  inv1 =inputs[i] / 2 + 0.5\n",
        "  TV.utils.save_image(inv1, join(paths.CIFAR10_CW_LOGS,'%d_original.png' %(i)))\n",
        "  inv2 = adversarial_examples[i] / 2 + 0.5\n",
        "  TV.utils.save_image(inv2, join(paths.CIFAR10_CW_LOGS,'%d_adversarial.png' %(i)))\n",
        "  list_img = [inv1, inv2]\n",
        "  grid = utils.make_grid(list_img)\n",
        "  show(grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "fGs6Ihqb_P2p",
        "outputId": "96c7cd3f-afdc-4995-a83b-99e6a5660681"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MNIST"
      ],
      "metadata": {
        "id": "ArNX2BumLdbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "tPcm61r0sODv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.5]\n",
        "std= [0.5]\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST(root = '/content/MNIST', train=True, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(28),\n",
        "                               transforms.CenterCrop(28),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(mean, std),\n",
        "                           ])),batch_size=9, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST(root = '/content/MNIST', train=False, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(28),\n",
        "                               transforms.CenterCrop(28),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(0.5, 0.5),\n",
        "                           ])),batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "1a5jPFuHLckM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "A_M1MJz8sY1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, kernel_size=3, stride=2)\n",
        "        self.bn = nn.BatchNorm2d(20)\n",
        "        self.conv2 = nn.Conv2d(20, 10, kernel_size=3, stride=3)\n",
        "        self.bn2 = nn.BatchNorm2d(10)\n",
        "        self.fc1 = nn.Linear(10 * 4 * 4, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = x.view(-1, 10 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "model = MnistCNN()"
      ],
      "metadata": {
        "id": "m6yqZQb_MaBo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "XR2Hxw86sa8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) \n",
        "\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfBEBeY2MgjU",
        "outputId": "285c36ec-0587-4b08-c8fd-9c7d0a4ec25f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 0.210\n",
            "[1,   400] loss: 0.148\n",
            "[1,   600] loss: 0.088\n",
            "[1,   800] loss: 0.058\n",
            "[1,  1000] loss: 0.041\n",
            "[1,  1200] loss: 0.034\n",
            "[1,  1400] loss: 0.027\n",
            "[1,  1600] loss: 0.027\n",
            "[1,  1800] loss: 0.025\n",
            "[1,  2000] loss: 0.020\n",
            "[1,  2200] loss: 0.022\n",
            "[1,  2400] loss: 0.022\n",
            "[1,  2600] loss: 0.020\n",
            "[1,  2800] loss: 0.015\n",
            "[1,  3000] loss: 0.018\n",
            "[1,  3200] loss: 0.018\n",
            "[1,  3400] loss: 0.018\n",
            "[1,  3600] loss: 0.018\n",
            "[1,  3800] loss: 0.017\n",
            "[1,  4000] loss: 0.017\n",
            "[1,  4200] loss: 0.016\n",
            "[1,  4400] loss: 0.016\n",
            "[1,  4600] loss: 0.013\n",
            "[1,  4800] loss: 0.015\n",
            "[1,  5000] loss: 0.012\n",
            "[1,  5200] loss: 0.014\n",
            "[1,  5400] loss: 0.014\n",
            "[1,  5600] loss: 0.012\n",
            "[1,  5800] loss: 0.014\n",
            "[1,  6000] loss: 0.014\n",
            "[1,  6200] loss: 0.015\n",
            "[1,  6400] loss: 0.013\n",
            "[1,  6600] loss: 0.012\n",
            "[2,   200] loss: 0.013\n",
            "[2,   400] loss: 0.011\n",
            "[2,   600] loss: 0.012\n",
            "[2,   800] loss: 0.013\n",
            "[2,  1000] loss: 0.012\n",
            "[2,  1200] loss: 0.010\n",
            "[2,  1400] loss: 0.011\n",
            "[2,  1600] loss: 0.014\n",
            "[2,  1800] loss: 0.012\n",
            "[2,  2000] loss: 0.011\n",
            "[2,  2200] loss: 0.012\n",
            "[2,  2400] loss: 0.010\n",
            "[2,  2600] loss: 0.013\n",
            "[2,  2800] loss: 0.012\n",
            "[2,  3000] loss: 0.009\n",
            "[2,  3200] loss: 0.010\n",
            "[2,  3400] loss: 0.008\n",
            "[2,  3600] loss: 0.010\n",
            "[2,  3800] loss: 0.013\n",
            "[2,  4000] loss: 0.010\n",
            "[2,  4200] loss: 0.010\n",
            "[2,  4400] loss: 0.009\n",
            "[2,  4600] loss: 0.011\n",
            "[2,  4800] loss: 0.010\n",
            "[2,  5000] loss: 0.010\n",
            "[2,  5200] loss: 0.009\n",
            "[2,  5400] loss: 0.008\n",
            "[2,  5600] loss: 0.009\n",
            "[2,  5800] loss: 0.011\n",
            "[2,  6000] loss: 0.008\n",
            "[2,  6200] loss: 0.009\n",
            "[2,  6400] loss: 0.011\n",
            "[2,  6600] loss: 0.011\n",
            "[3,   200] loss: 0.008\n",
            "[3,   400] loss: 0.009\n",
            "[3,   600] loss: 0.009\n",
            "[3,   800] loss: 0.008\n",
            "[3,  1000] loss: 0.008\n",
            "[3,  1200] loss: 0.009\n",
            "[3,  1400] loss: 0.008\n",
            "[3,  1600] loss: 0.008\n",
            "[3,  1800] loss: 0.006\n",
            "[3,  2000] loss: 0.008\n",
            "[3,  2200] loss: 0.009\n",
            "[3,  2400] loss: 0.008\n",
            "[3,  2600] loss: 0.008\n",
            "[3,  2800] loss: 0.008\n",
            "[3,  3000] loss: 0.009\n",
            "[3,  3200] loss: 0.008\n",
            "[3,  3400] loss: 0.008\n",
            "[3,  3600] loss: 0.009\n",
            "[3,  3800] loss: 0.007\n",
            "[3,  4000] loss: 0.008\n",
            "[3,  4200] loss: 0.010\n",
            "[3,  4400] loss: 0.008\n",
            "[3,  4600] loss: 0.008\n",
            "[3,  4800] loss: 0.009\n",
            "[3,  5000] loss: 0.006\n",
            "[3,  5200] loss: 0.010\n",
            "[3,  5400] loss: 0.008\n",
            "[3,  5600] loss: 0.008\n",
            "[3,  5800] loss: 0.009\n",
            "[3,  6000] loss: 0.007\n",
            "[3,  6200] loss: 0.009\n",
            "[3,  6400] loss: 0.007\n",
            "[3,  6600] loss: 0.007\n",
            "[4,   200] loss: 0.007\n",
            "[4,   400] loss: 0.008\n",
            "[4,   600] loss: 0.007\n",
            "[4,   800] loss: 0.007\n",
            "[4,  1000] loss: 0.009\n",
            "[4,  1200] loss: 0.006\n",
            "[4,  1400] loss: 0.007\n",
            "[4,  1600] loss: 0.007\n",
            "[4,  1800] loss: 0.007\n",
            "[4,  2000] loss: 0.008\n",
            "[4,  2200] loss: 0.007\n",
            "[4,  2400] loss: 0.007\n",
            "[4,  2600] loss: 0.006\n",
            "[4,  2800] loss: 0.006\n",
            "[4,  3000] loss: 0.008\n",
            "[4,  3200] loss: 0.006\n",
            "[4,  3400] loss: 0.006\n",
            "[4,  3600] loss: 0.006\n",
            "[4,  3800] loss: 0.008\n",
            "[4,  4000] loss: 0.006\n",
            "[4,  4200] loss: 0.007\n",
            "[4,  4400] loss: 0.005\n",
            "[4,  4600] loss: 0.007\n",
            "[4,  4800] loss: 0.007\n",
            "[4,  5000] loss: 0.006\n",
            "[4,  5200] loss: 0.007\n",
            "[4,  5400] loss: 0.007\n",
            "[4,  5600] loss: 0.007\n",
            "[4,  5800] loss: 0.009\n",
            "[4,  6000] loss: 0.007\n",
            "[4,  6200] loss: 0.007\n",
            "[4,  6400] loss: 0.007\n",
            "[4,  6600] loss: 0.007\n",
            "[5,   200] loss: 0.006\n",
            "[5,   400] loss: 0.006\n",
            "[5,   600] loss: 0.007\n",
            "[5,   800] loss: 0.006\n",
            "[5,  1000] loss: 0.005\n",
            "[5,  1200] loss: 0.005\n",
            "[5,  1400] loss: 0.006\n",
            "[5,  1600] loss: 0.006\n",
            "[5,  1800] loss: 0.006\n",
            "[5,  2000] loss: 0.005\n",
            "[5,  2200] loss: 0.007\n",
            "[5,  2400] loss: 0.007\n",
            "[5,  2600] loss: 0.006\n",
            "[5,  2800] loss: 0.005\n",
            "[5,  3000] loss: 0.005\n",
            "[5,  3200] loss: 0.007\n",
            "[5,  3400] loss: 0.006\n",
            "[5,  3600] loss: 0.008\n",
            "[5,  3800] loss: 0.005\n",
            "[5,  4000] loss: 0.005\n",
            "[5,  4200] loss: 0.006\n",
            "[5,  4400] loss: 0.006\n",
            "[5,  4600] loss: 0.007\n",
            "[5,  4800] loss: 0.007\n",
            "[5,  5000] loss: 0.006\n",
            "[5,  5200] loss: 0.005\n",
            "[5,  5400] loss: 0.007\n",
            "[5,  5600] loss: 0.005\n",
            "[5,  5800] loss: 0.006\n",
            "[5,  6000] loss: 0.005\n",
            "[5,  6200] loss: 0.007\n",
            "[5,  6400] loss: 0.007\n",
            "[5,  6600] loss: 0.008\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/TFG/MNIST/MNIST_net.pth'\n",
        "torch.save(model.state_dict(), paths.MNIST_DIR)"
      ],
      "metadata": {
        "id": "eeLlij7MQ2_A"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attack"
      ],
      "metadata": {
        "id": "Kza07Wrfxq3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(paths.MNIST_DIR))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "inputs_box = (min((0 - m) / s for m, s in zip(mean, std)),\n",
        "              max((1 - m) / s for m, s in zip(mean, std)))\n",
        "\n",
        "# a targeted adversary\n",
        "adversary = L2Adversary(targeted=True,\n",
        "                           k=0.0,\n",
        "                           search_steps=10,\n",
        "                           box=inputs_box,\n",
        "                           learning_rate=5e-4)\n",
        "\n",
        "\n",
        "dataiter = iter(train_loader) #inputs images\n",
        "inputs, _ = dataiter.next()\n",
        "\n",
        "inputs.to(device)\n",
        "\n",
        "target_class_idx = 3\n",
        "attack_targets = torch.ones(inputs.size(0)) * target_class_idx #target one-hot encoded\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "adversarial_examples = adversary.attack(model, inputs, attack_targets, NUM_CLASSES)\n",
        "assert isinstance(adversarial_examples, torch.FloatTensor)\n",
        "assert adversarial_examples.size() == inputs.size()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YwDNGhGR56I",
        "outputId": "fe1a156d-a7ad-4f5d-f8f7-94158e4461b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using scale consts: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
            "batch 0 loss: 0.11988122016191483\n",
            "batch 10 loss: 0.11987428367137909\n",
            "batch 20 loss: 0.119864821434021\n",
            "batch 30 loss: 0.11986134946346283\n",
            "batch 40 loss: 0.11986072361469269\n",
            "batch 50 loss: 0.1198604628443718\n",
            "batch 60 loss: 0.11986035108566284\n",
            "batch 70 loss: 0.11986032128334045\n",
            "batch 80 loss: 0.11986030638217926\n",
            "batch 90 loss: 0.11986029148101807\n",
            "batch 100 loss: 0.11986029148101807\n",
            "batch 110 loss: 0.11986029148101807\n",
            "batch 120 loss: 0.11986029893159866\n",
            "batch 130 loss: 0.11986029148101807\n",
            "batch 140 loss: 0.11986029148101807\n",
            "batch 150 loss: 0.11986029148101807\n",
            "batch 160 loss: 0.11986028403043747\n",
            "batch 170 loss: 0.11986029148101807\n",
            "batch 180 loss: 0.11986029148101807\n",
            "batch 190 loss: 0.11986029148101807\n",
            "batch 200 loss: 0.11986029893159866\n",
            "Using scale consts: [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
            "batch 0 loss: 1.1984145641326904\n",
            "batch 10 loss: 1.1972054243087769\n",
            "batch 20 loss: 1.1969527006149292\n",
            "batch 30 loss: 1.196850061416626\n",
            "batch 40 loss: 1.1967992782592773\n",
            "batch 50 loss: 1.196773886680603\n",
            "batch 60 loss: 1.1967592239379883\n",
            "batch 70 loss: 1.1967501640319824\n",
            "batch 80 loss: 1.1967443227767944\n",
            "batch 90 loss: 1.1967401504516602\n",
            "batch 100 loss: 1.1967370510101318\n",
            "batch 110 loss: 1.1967347860336304\n",
            "batch 120 loss: 1.196732759475708\n",
            "batch 130 loss: 1.1967313289642334\n",
            "batch 140 loss: 1.196730136871338\n",
            "batch 150 loss: 1.1967289447784424\n",
            "batch 160 loss: 1.196727991104126\n",
            "batch 170 loss: 1.1967273950576782\n",
            "batch 180 loss: 1.1967265605926514\n",
            "batch 190 loss: 1.1967260837554932\n",
            "batch 200 loss: 1.196725606918335\n",
            "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "batch 0 loss: 11.948718070983887\n",
            "batch 10 loss: 11.874287605285645\n",
            "batch 20 loss: 11.849318504333496\n",
            "batch 30 loss: 11.837238311767578\n",
            "batch 40 loss: 11.829994201660156\n",
            "batch 50 loss: 11.825302124023438\n",
            "batch 60 loss: 11.821868896484375\n",
            "batch 70 loss: 11.819158554077148\n",
            "batch 80 loss: 11.81692123413086\n",
            "batch 90 loss: 11.814971923828125\n",
            "batch 100 loss: 11.81322956085205\n",
            "batch 110 loss: 11.811702728271484\n",
            "batch 120 loss: 11.810327529907227\n",
            "batch 130 loss: 11.809069633483887\n",
            "batch 140 loss: 11.807923316955566\n",
            "batch 150 loss: 11.806766510009766\n",
            "batch 160 loss: 11.805682182312012\n",
            "batch 170 loss: 11.804737091064453\n",
            "batch 180 loss: 11.803865432739258\n",
            "batch 190 loss: 11.803054809570312\n",
            "batch 200 loss: 11.802300453186035\n",
            "batch 210 loss: 11.801589965820312\n",
            "batch 220 loss: 11.800926208496094\n",
            "batch 230 loss: 11.800297737121582\n",
            "batch 240 loss: 11.799703598022461\n",
            "batch 250 loss: 11.79914665222168\n",
            "batch 260 loss: 11.798614501953125\n",
            "batch 270 loss: 11.798107147216797\n",
            "batch 280 loss: 11.79763412475586\n",
            "batch 290 loss: 11.79718017578125\n",
            "batch 300 loss: 11.796751976013184\n",
            "batch 310 loss: 11.796346664428711\n",
            "batch 320 loss: 11.795957565307617\n",
            "batch 330 loss: 11.795589447021484\n",
            "batch 340 loss: 11.79524040222168\n",
            "batch 350 loss: 11.794906616210938\n",
            "batch 360 loss: 11.794588088989258\n",
            "batch 370 loss: 11.79428482055664\n",
            "batch 380 loss: 11.793998718261719\n",
            "batch 390 loss: 11.793722152709961\n",
            "batch 400 loss: 11.793460845947266\n",
            "batch 410 loss: 11.793206214904785\n",
            "batch 420 loss: 11.792972564697266\n",
            "batch 430 loss: 11.792740821838379\n",
            "batch 440 loss: 11.792526245117188\n",
            "batch 450 loss: 11.792314529418945\n",
            "batch 460 loss: 11.792118072509766\n",
            "batch 470 loss: 11.791929244995117\n",
            "batch 480 loss: 11.791746139526367\n",
            "batch 490 loss: 11.791576385498047\n",
            "batch 500 loss: 11.791406631469727\n",
            "batch 510 loss: 11.791254043579102\n",
            "batch 520 loss: 11.791101455688477\n",
            "batch 530 loss: 11.79095458984375\n",
            "batch 540 loss: 11.790817260742188\n",
            "batch 550 loss: 11.790681838989258\n",
            "batch 560 loss: 11.790555953979492\n",
            "batch 570 loss: 11.790431022644043\n",
            "batch 580 loss: 11.790315628051758\n",
            "batch 590 loss: 11.790201187133789\n",
            "batch 600 loss: 11.79008960723877\n",
            "batch 610 loss: 11.789985656738281\n",
            "batch 620 loss: 11.789886474609375\n",
            "batch 630 loss: 11.789793014526367\n",
            "batch 640 loss: 11.789698600769043\n",
            "batch 650 loss: 11.78961181640625\n",
            "batch 660 loss: 11.789525985717773\n",
            "batch 670 loss: 11.789441108703613\n",
            "batch 680 loss: 11.78936767578125\n",
            "batch 690 loss: 11.789285659790039\n",
            "batch 700 loss: 11.789213180541992\n",
            "Using scale consts: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "batch 0 loss: 116.24040222167969\n",
            "batch 10 loss: 114.42893981933594\n",
            "batch 20 loss: 112.66635131835938\n",
            "batch 30 loss: 111.35211181640625\n",
            "batch 40 loss: 110.39035034179688\n",
            "batch 50 loss: 109.6183853149414\n",
            "batch 60 loss: 108.98727416992188\n",
            "batch 70 loss: 108.47148132324219\n",
            "batch 80 loss: 108.04214477539062\n",
            "batch 90 loss: 107.65711212158203\n",
            "batch 100 loss: 107.31029510498047\n",
            "batch 110 loss: 107.00658416748047\n",
            "batch 120 loss: 106.74177551269531\n",
            "batch 130 loss: 106.5016098022461\n",
            "batch 140 loss: 106.24845886230469\n",
            "batch 150 loss: 105.99954223632812\n",
            "batch 160 loss: 105.77104187011719\n",
            "batch 170 loss: 105.56716918945312\n",
            "batch 180 loss: 105.37174987792969\n",
            "batch 190 loss: 105.18407440185547\n",
            "batch 200 loss: 105.0123291015625\n",
            "batch 210 loss: 104.8536376953125\n",
            "batch 220 loss: 104.70729064941406\n",
            "batch 230 loss: 104.5541763305664\n",
            "batch 240 loss: 104.38294982910156\n",
            "batch 250 loss: 104.21966552734375\n",
            "batch 260 loss: 104.06837463378906\n",
            "batch 270 loss: 103.92789459228516\n",
            "batch 280 loss: 103.79811096191406\n",
            "batch 290 loss: 103.68063354492188\n",
            "batch 300 loss: 103.56745147705078\n",
            "batch 310 loss: 103.46025848388672\n",
            "batch 320 loss: 103.35752868652344\n",
            "batch 330 loss: 103.25953674316406\n",
            "batch 340 loss: 103.16563415527344\n",
            "batch 350 loss: 103.07451629638672\n",
            "batch 360 loss: 102.97042846679688\n",
            "batch 370 loss: 102.8624038696289\n",
            "batch 380 loss: 102.75813293457031\n",
            "batch 390 loss: 102.65524291992188\n",
            "batch 400 loss: 102.55705261230469\n",
            "batch 410 loss: 102.46327209472656\n",
            "batch 420 loss: 102.37242126464844\n",
            "batch 430 loss: 102.28414916992188\n",
            "batch 440 loss: 102.19645690917969\n",
            "batch 450 loss: 102.11164855957031\n",
            "batch 460 loss: 102.02965545654297\n",
            "batch 470 loss: 101.94876098632812\n",
            "batch 480 loss: 101.87211608886719\n",
            "batch 490 loss: 101.79827880859375\n",
            "batch 500 loss: 101.72700500488281\n",
            "batch 510 loss: 101.65800476074219\n",
            "batch 520 loss: 101.59146118164062\n",
            "batch 530 loss: 101.527099609375\n",
            "batch 540 loss: 101.4652099609375\n",
            "batch 550 loss: 101.40480041503906\n",
            "batch 560 loss: 101.34658813476562\n",
            "batch 570 loss: 101.28924560546875\n",
            "batch 580 loss: 101.23381042480469\n",
            "batch 590 loss: 101.17788696289062\n",
            "batch 600 loss: 101.11995697021484\n",
            "batch 610 loss: 101.05677795410156\n",
            "batch 620 loss: 100.99594116210938\n",
            "batch 630 loss: 100.897705078125\n",
            "batch 640 loss: 100.78230285644531\n",
            "batch 650 loss: 100.68224334716797\n",
            "batch 660 loss: 100.59391784667969\n",
            "batch 670 loss: 100.51467895507812\n",
            "batch 680 loss: 100.44229125976562\n",
            "batch 690 loss: 100.37548065185547\n",
            "batch 700 loss: 100.31253814697266\n",
            "batch 710 loss: 100.25332641601562\n",
            "batch 720 loss: 100.19705200195312\n",
            "batch 730 loss: 100.1427001953125\n",
            "batch 740 loss: 100.09103393554688\n",
            "batch 750 loss: 100.04110717773438\n",
            "batch 760 loss: 99.99407958984375\n",
            "batch 770 loss: 99.94783782958984\n",
            "batch 780 loss: 99.9052963256836\n",
            "batch 790 loss: 99.86434936523438\n",
            "batch 800 loss: 99.82524871826172\n",
            "batch 810 loss: 99.78739166259766\n",
            "batch 820 loss: 99.75054168701172\n",
            "batch 830 loss: 99.7146224975586\n",
            "batch 840 loss: 99.6801528930664\n",
            "batch 850 loss: 99.64727020263672\n",
            "batch 860 loss: 99.61544036865234\n",
            "batch 870 loss: 99.58106231689453\n",
            "batch 880 loss: 99.54782104492188\n",
            "batch 890 loss: 99.51335906982422\n",
            "batch 900 loss: 99.47441101074219\n",
            "batch 910 loss: 99.43760681152344\n",
            "batch 920 loss: 99.40402221679688\n",
            "batch 930 loss: 99.36970520019531\n",
            "batch 940 loss: 99.33374786376953\n",
            "batch 950 loss: 99.30104064941406\n",
            "batch 960 loss: 99.27127075195312\n",
            "batch 970 loss: 99.24369812011719\n",
            "batch 980 loss: 99.21734619140625\n",
            "batch 990 loss: 99.19203186035156\n",
            "Using scale consts: [10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]\n",
            "batch 0 loss: 835.6843872070312\n",
            "batch 10 loss: 810.6318359375\n",
            "batch 20 loss: 780.821533203125\n",
            "batch 30 loss: 753.4241943359375\n",
            "batch 40 loss: 728.367431640625\n",
            "batch 50 loss: 705.4912109375\n",
            "batch 60 loss: 684.891845703125\n",
            "batch 70 loss: 668.247314453125\n",
            "batch 80 loss: 652.5885009765625\n",
            "batch 90 loss: 637.7064208984375\n",
            "batch 100 loss: 623.45654296875\n",
            "batch 110 loss: 609.8961181640625\n",
            "batch 120 loss: 597.0755615234375\n",
            "batch 130 loss: 584.52099609375\n",
            "batch 140 loss: 572.6547241210938\n",
            "batch 150 loss: 561.1046142578125\n",
            "batch 160 loss: 550.3046875\n",
            "batch 170 loss: 539.7681274414062\n",
            "batch 180 loss: 529.53759765625\n",
            "batch 190 loss: 520.2032470703125\n",
            "batch 200 loss: 511.75299072265625\n",
            "batch 210 loss: 503.3043518066406\n",
            "batch 220 loss: 495.207275390625\n",
            "batch 230 loss: 487.24774169921875\n",
            "batch 240 loss: 479.80291748046875\n",
            "batch 250 loss: 472.3575134277344\n",
            "batch 260 loss: 464.923095703125\n",
            "batch 270 loss: 457.68548583984375\n",
            "batch 280 loss: 450.35125732421875\n",
            "batch 290 loss: 443.2198181152344\n",
            "batch 300 loss: 436.5518493652344\n",
            "batch 310 loss: 430.19671630859375\n",
            "batch 320 loss: 423.9929504394531\n",
            "batch 330 loss: 418.621337890625\n",
            "batch 340 loss: 415.08001708984375\n",
            "batch 350 loss: 411.2320556640625\n",
            "batch 360 loss: 407.4537353515625\n",
            "batch 370 loss: 403.77398681640625\n",
            "batch 380 loss: 400.05633544921875\n",
            "batch 390 loss: 396.35009765625\n",
            "batch 400 loss: 392.6400146484375\n",
            "batch 410 loss: 388.9498291015625\n",
            "batch 420 loss: 385.2276306152344\n",
            "batch 430 loss: 381.6216735839844\n",
            "batch 440 loss: 378.1131896972656\n",
            "batch 450 loss: 374.5935974121094\n",
            "batch 460 loss: 371.1663818359375\n",
            "batch 470 loss: 367.7896423339844\n",
            "batch 480 loss: 364.3089294433594\n",
            "batch 490 loss: 360.9112548828125\n",
            "batch 500 loss: 357.4783020019531\n",
            "batch 510 loss: 354.12982177734375\n",
            "batch 520 loss: 350.7908935546875\n",
            "batch 530 loss: 347.5205993652344\n",
            "batch 540 loss: 344.2413330078125\n",
            "batch 550 loss: 341.08428955078125\n",
            "batch 560 loss: 337.96343994140625\n",
            "batch 570 loss: 334.7567138671875\n",
            "batch 580 loss: 331.5477294921875\n",
            "batch 590 loss: 328.36669921875\n",
            "batch 600 loss: 325.2381591796875\n",
            "batch 610 loss: 322.14666748046875\n",
            "batch 620 loss: 319.08441162109375\n",
            "batch 630 loss: 316.110107421875\n",
            "batch 640 loss: 313.19573974609375\n",
            "batch 650 loss: 310.2140197753906\n",
            "batch 660 loss: 307.31756591796875\n",
            "batch 670 loss: 304.5096740722656\n",
            "batch 680 loss: 301.63897705078125\n",
            "batch 690 loss: 298.8309020996094\n",
            "batch 700 loss: 296.1213684082031\n",
            "batch 710 loss: 293.48260498046875\n",
            "batch 720 loss: 290.6880798339844\n",
            "batch 730 loss: 287.75970458984375\n",
            "batch 740 loss: 284.83447265625\n",
            "batch 750 loss: 281.8667297363281\n",
            "batch 760 loss: 278.85162353515625\n",
            "batch 770 loss: 275.832275390625\n",
            "batch 780 loss: 272.749755859375\n",
            "batch 790 loss: 269.6998291015625\n",
            "batch 800 loss: 266.6963806152344\n",
            "batch 810 loss: 263.78277587890625\n",
            "batch 820 loss: 260.9488525390625\n",
            "batch 830 loss: 258.1493835449219\n",
            "batch 840 loss: 255.3858184814453\n",
            "batch 850 loss: 252.733154296875\n",
            "batch 860 loss: 250.13876342773438\n",
            "batch 870 loss: 247.63345336914062\n",
            "batch 880 loss: 245.84979248046875\n",
            "batch 890 loss: 244.1508331298828\n",
            "batch 900 loss: 242.5934600830078\n",
            "batch 910 loss: 241.057861328125\n",
            "batch 920 loss: 239.56759643554688\n",
            "batch 930 loss: 238.075439453125\n",
            "batch 940 loss: 236.6470947265625\n",
            "batch 950 loss: 235.26808166503906\n",
            "batch 960 loss: 233.9002685546875\n",
            "batch 970 loss: 232.58740234375\n",
            "batch 980 loss: 231.2161865234375\n",
            "batch 990 loss: 229.9080810546875\n",
            "Using scale consts: [5.5, 5.5, 100.0, 100.0, 5.5, 5.5, 5.5, 5.5, 100.0]\n",
            "batch 0 loss: 857.5548095703125\n",
            "batch 10 loss: 790.9183959960938\n",
            "batch 20 loss: 704.108642578125\n",
            "batch 30 loss: 624.4375\n",
            "batch 40 loss: 552.4909057617188\n",
            "batch 50 loss: 499.83135986328125\n",
            "batch 60 loss: 457.6871643066406\n",
            "batch 70 loss: 422.79974365234375\n",
            "batch 80 loss: 403.05487060546875\n",
            "batch 90 loss: 395.1817626953125\n",
            "batch 100 loss: 387.0770568847656\n",
            "batch 110 loss: 379.18511962890625\n",
            "batch 120 loss: 371.73016357421875\n",
            "batch 130 loss: 364.6820068359375\n",
            "batch 140 loss: 357.95806884765625\n",
            "batch 150 loss: 351.4881896972656\n",
            "batch 160 loss: 345.5286865234375\n",
            "batch 170 loss: 339.8650207519531\n",
            "batch 180 loss: 334.550537109375\n",
            "batch 190 loss: 329.27685546875\n",
            "batch 200 loss: 324.23699951171875\n",
            "batch 210 loss: 319.39434814453125\n",
            "batch 220 loss: 314.85772705078125\n",
            "batch 230 loss: 310.12164306640625\n",
            "batch 240 loss: 305.7313537597656\n",
            "batch 250 loss: 301.30023193359375\n",
            "batch 260 loss: 296.98028564453125\n",
            "batch 270 loss: 292.7934875488281\n",
            "batch 280 loss: 288.7106628417969\n",
            "batch 290 loss: 284.6533203125\n",
            "batch 300 loss: 280.6705627441406\n",
            "batch 310 loss: 276.60546875\n",
            "batch 320 loss: 272.7541809082031\n",
            "batch 330 loss: 268.877685546875\n",
            "batch 340 loss: 265.13848876953125\n",
            "batch 350 loss: 261.4952392578125\n",
            "batch 360 loss: 257.6666259765625\n",
            "batch 370 loss: 253.97872924804688\n",
            "batch 380 loss: 250.17515563964844\n",
            "batch 390 loss: 246.56939697265625\n",
            "batch 400 loss: 243.10482788085938\n",
            "batch 410 loss: 239.2423095703125\n",
            "batch 420 loss: 235.44171142578125\n",
            "batch 430 loss: 231.7938995361328\n",
            "batch 440 loss: 228.19606018066406\n",
            "batch 450 loss: 224.6298370361328\n",
            "batch 460 loss: 221.15945434570312\n",
            "batch 470 loss: 217.74053955078125\n",
            "batch 480 loss: 214.3079071044922\n",
            "batch 490 loss: 211.15887451171875\n",
            "batch 500 loss: 207.86956787109375\n",
            "batch 510 loss: 204.45681762695312\n",
            "batch 520 loss: 200.98687744140625\n",
            "batch 530 loss: 200.35720825195312\n",
            "batch 540 loss: 199.9730682373047\n",
            "batch 550 loss: 199.56846618652344\n",
            "batch 560 loss: 199.02780151367188\n",
            "batch 570 loss: 198.58273315429688\n",
            "batch 580 loss: 198.32785034179688\n",
            "batch 590 loss: 197.9569091796875\n",
            "batch 600 loss: 197.65457153320312\n",
            "batch 610 loss: 197.37109375\n",
            "batch 620 loss: 197.02401733398438\n",
            "batch 630 loss: 196.7737274169922\n",
            "batch 640 loss: 196.47396850585938\n",
            "batch 650 loss: 196.2233428955078\n",
            "batch 660 loss: 195.91000366210938\n",
            "batch 670 loss: 195.57150268554688\n",
            "batch 680 loss: 195.373046875\n",
            "batch 690 loss: 195.13491821289062\n",
            "batch 700 loss: 194.77230834960938\n",
            "batch 710 loss: 194.5755615234375\n",
            "batch 720 loss: 194.2357177734375\n",
            "batch 730 loss: 194.02169799804688\n",
            "batch 740 loss: 193.77679443359375\n",
            "batch 750 loss: 193.41571044921875\n",
            "batch 760 loss: 193.30978393554688\n",
            "batch 770 loss: 193.03012084960938\n",
            "batch 780 loss: 192.72340393066406\n",
            "batch 790 loss: 192.5568389892578\n",
            "batch 800 loss: 192.26779174804688\n",
            "batch 810 loss: 192.02059936523438\n",
            "batch 820 loss: 191.86761474609375\n",
            "batch 830 loss: 191.5762939453125\n",
            "batch 840 loss: 191.34793090820312\n",
            "batch 850 loss: 191.15036010742188\n",
            "batch 860 loss: 190.91152954101562\n",
            "batch 870 loss: 190.7065887451172\n",
            "batch 880 loss: 190.5083770751953\n",
            "batch 890 loss: 190.302978515625\n",
            "batch 900 loss: 190.09683227539062\n",
            "batch 910 loss: 189.80618286132812\n",
            "batch 920 loss: 189.7093505859375\n",
            "batch 930 loss: 189.4447479248047\n",
            "batch 940 loss: 189.19781494140625\n",
            "batch 950 loss: 189.15138244628906\n",
            "batch 960 loss: 188.8966522216797\n",
            "batch 970 loss: 188.66091918945312\n",
            "batch 980 loss: 188.52450561523438\n",
            "batch 990 loss: 188.27874755859375\n",
            "Using scale consts: [3.25, 3.25, 55.0, 55.0, 3.25, 3.25, 3.25, 3.25, 55.0]\n",
            "batch 0 loss: 188.11495971679688\n",
            "batch 10 loss: 187.9243927001953\n",
            "batch 20 loss: 187.69488525390625\n",
            "batch 30 loss: 187.50509643554688\n",
            "batch 40 loss: 187.33042907714844\n",
            "batch 50 loss: 187.13323974609375\n",
            "batch 60 loss: 186.97132873535156\n",
            "batch 70 loss: 186.7962646484375\n",
            "batch 80 loss: 186.66949462890625\n",
            "batch 90 loss: 186.4539031982422\n",
            "batch 100 loss: 186.2912139892578\n",
            "batch 110 loss: 186.15679931640625\n",
            "batch 120 loss: 185.95213317871094\n",
            "batch 130 loss: 185.80874633789062\n",
            "batch 140 loss: 185.6005096435547\n",
            "batch 150 loss: 185.47421264648438\n",
            "batch 160 loss: 185.31008911132812\n",
            "batch 170 loss: 185.1185760498047\n",
            "batch 180 loss: 184.9923858642578\n",
            "batch 190 loss: 184.81227111816406\n",
            "batch 200 loss: 184.66592407226562\n",
            "batch 210 loss: 184.52481079101562\n",
            "batch 220 loss: 184.36514282226562\n",
            "batch 230 loss: 184.20083618164062\n",
            "batch 240 loss: 184.0687713623047\n",
            "batch 250 loss: 183.92088317871094\n",
            "batch 260 loss: 183.75436401367188\n",
            "batch 270 loss: 183.60714721679688\n",
            "batch 280 loss: 183.45408630371094\n",
            "batch 290 loss: 183.31768798828125\n",
            "batch 300 loss: 183.19888305664062\n",
            "batch 310 loss: 183.00601196289062\n",
            "batch 320 loss: 182.89137268066406\n",
            "batch 330 loss: 182.76821899414062\n",
            "batch 340 loss: 182.61880493164062\n",
            "batch 350 loss: 182.47463989257812\n",
            "batch 360 loss: 182.32693481445312\n",
            "batch 370 loss: 182.18911743164062\n",
            "batch 380 loss: 182.05812072753906\n",
            "batch 390 loss: 181.8997039794922\n",
            "batch 400 loss: 181.77203369140625\n",
            "batch 410 loss: 181.65170288085938\n",
            "batch 420 loss: 181.55337524414062\n",
            "batch 430 loss: 181.39755249023438\n",
            "batch 440 loss: 181.25885009765625\n",
            "batch 450 loss: 181.11427307128906\n",
            "batch 460 loss: 180.98049926757812\n",
            "batch 470 loss: 180.89430236816406\n",
            "batch 480 loss: 180.7884521484375\n",
            "batch 490 loss: 180.611083984375\n",
            "batch 500 loss: 180.50364685058594\n",
            "batch 510 loss: 180.34861755371094\n",
            "batch 520 loss: 180.25051879882812\n",
            "batch 530 loss: 180.12451171875\n",
            "batch 540 loss: 179.9864501953125\n",
            "batch 550 loss: 179.90464782714844\n",
            "batch 560 loss: 179.78176879882812\n",
            "batch 570 loss: 179.64523315429688\n",
            "batch 580 loss: 179.54393005371094\n",
            "batch 590 loss: 179.40704345703125\n",
            "batch 600 loss: 179.29273986816406\n",
            "batch 610 loss: 179.18331909179688\n",
            "batch 620 loss: 179.07131958007812\n",
            "batch 630 loss: 178.95437622070312\n",
            "batch 640 loss: 178.84906005859375\n",
            "batch 650 loss: 178.81137084960938\n",
            "batch 660 loss: 178.63949584960938\n",
            "batch 670 loss: 178.5186767578125\n",
            "batch 680 loss: 178.3982391357422\n",
            "batch 690 loss: 178.3047637939453\n",
            "batch 700 loss: 178.20608520507812\n",
            "batch 710 loss: 178.1121826171875\n",
            "batch 720 loss: 178.022705078125\n",
            "batch 730 loss: 177.92474365234375\n",
            "batch 740 loss: 177.77064514160156\n",
            "batch 750 loss: 177.73033142089844\n",
            "batch 760 loss: 177.6153106689453\n",
            "batch 770 loss: 177.49127197265625\n",
            "batch 780 loss: 177.40867614746094\n",
            "batch 790 loss: 177.29685974121094\n",
            "batch 800 loss: 177.1883544921875\n",
            "batch 810 loss: 177.1036376953125\n",
            "batch 820 loss: 176.9920196533203\n",
            "batch 830 loss: 176.89271545410156\n",
            "batch 840 loss: 176.81288146972656\n",
            "batch 850 loss: 176.69882202148438\n",
            "batch 860 loss: 176.6182861328125\n",
            "batch 870 loss: 176.53030395507812\n",
            "batch 880 loss: 176.43417358398438\n",
            "batch 890 loss: 176.35614013671875\n",
            "batch 900 loss: 176.241455078125\n",
            "batch 910 loss: 176.15745544433594\n",
            "batch 920 loss: 176.08456420898438\n",
            "batch 930 loss: 176.046142578125\n",
            "batch 940 loss: 175.8985595703125\n",
            "batch 950 loss: 175.830078125\n",
            "batch 960 loss: 175.7087860107422\n",
            "batch 970 loss: 175.6302947998047\n",
            "batch 980 loss: 175.5462188720703\n",
            "batch 990 loss: 175.46392822265625\n",
            "Using scale consts: [2.125, 2.125, 32.5, 32.5, 2.125, 2.125, 2.125, 2.125, 32.5]\n",
            "batch 0 loss: 175.3888702392578\n",
            "batch 10 loss: 175.27572631835938\n",
            "batch 20 loss: 175.1652374267578\n",
            "batch 30 loss: 175.08126831054688\n",
            "batch 40 loss: 174.98248291015625\n",
            "batch 50 loss: 174.8992156982422\n",
            "batch 60 loss: 174.8289794921875\n",
            "batch 70 loss: 174.75198364257812\n",
            "batch 80 loss: 174.65890502929688\n",
            "batch 90 loss: 174.56027221679688\n",
            "batch 100 loss: 174.49481201171875\n",
            "batch 110 loss: 174.39028930664062\n",
            "batch 120 loss: 174.2998504638672\n",
            "batch 130 loss: 174.23233032226562\n",
            "batch 140 loss: 174.20164489746094\n",
            "batch 150 loss: 174.10986328125\n",
            "batch 160 loss: 174.00762939453125\n",
            "batch 170 loss: 173.91932678222656\n",
            "batch 180 loss: 173.82373046875\n",
            "batch 190 loss: 173.74331665039062\n",
            "batch 200 loss: 173.66793823242188\n",
            "batch 210 loss: 173.60812377929688\n",
            "batch 220 loss: 173.51983642578125\n",
            "batch 230 loss: 173.42721557617188\n",
            "batch 240 loss: 173.33331298828125\n",
            "batch 250 loss: 173.27114868164062\n",
            "batch 260 loss: 173.17721557617188\n",
            "batch 270 loss: 173.13446044921875\n",
            "batch 280 loss: 173.0428466796875\n",
            "batch 290 loss: 172.95590209960938\n",
            "batch 300 loss: 172.8892059326172\n",
            "batch 310 loss: 172.7991943359375\n",
            "batch 320 loss: 172.73619079589844\n",
            "batch 330 loss: 172.6582489013672\n",
            "batch 340 loss: 172.58843994140625\n",
            "batch 350 loss: 172.5143280029297\n",
            "batch 360 loss: 172.44216918945312\n",
            "batch 370 loss: 172.3763427734375\n",
            "batch 380 loss: 172.29225158691406\n",
            "batch 390 loss: 172.21876525878906\n",
            "batch 400 loss: 172.14035034179688\n",
            "batch 410 loss: 172.06622314453125\n",
            "batch 420 loss: 171.99017333984375\n",
            "batch 430 loss: 171.91543579101562\n",
            "batch 440 loss: 171.87017822265625\n",
            "batch 450 loss: 171.775634765625\n",
            "batch 460 loss: 171.6942138671875\n",
            "batch 470 loss: 171.63912963867188\n",
            "batch 480 loss: 171.57281494140625\n",
            "batch 490 loss: 171.50289916992188\n",
            "batch 500 loss: 171.41433715820312\n",
            "batch 510 loss: 171.33840942382812\n",
            "batch 520 loss: 171.2754364013672\n",
            "batch 530 loss: 171.19796752929688\n",
            "batch 540 loss: 171.11892700195312\n",
            "batch 550 loss: 171.055419921875\n",
            "batch 560 loss: 170.99148559570312\n",
            "batch 570 loss: 170.93812561035156\n",
            "batch 580 loss: 170.8499298095703\n",
            "batch 590 loss: 170.78298950195312\n",
            "batch 600 loss: 170.70291137695312\n",
            "batch 610 loss: 170.63296508789062\n",
            "batch 620 loss: 170.56480407714844\n",
            "batch 630 loss: 170.48936462402344\n",
            "batch 640 loss: 170.4293975830078\n",
            "batch 650 loss: 170.358154296875\n",
            "batch 660 loss: 170.28427124023438\n",
            "batch 670 loss: 170.2240753173828\n",
            "batch 680 loss: 170.14822387695312\n",
            "batch 690 loss: 170.08154296875\n",
            "batch 700 loss: 170.0284423828125\n",
            "batch 710 loss: 169.9861602783203\n",
            "batch 720 loss: 169.87857055664062\n",
            "batch 730 loss: 169.82470703125\n",
            "batch 740 loss: 169.74826049804688\n",
            "batch 750 loss: 169.68038940429688\n",
            "batch 760 loss: 169.63253784179688\n",
            "batch 770 loss: 169.53933715820312\n",
            "batch 780 loss: 169.4888916015625\n",
            "batch 790 loss: 169.41329956054688\n",
            "batch 800 loss: 169.3629608154297\n",
            "batch 810 loss: 169.27601623535156\n",
            "batch 820 loss: 169.21371459960938\n",
            "batch 830 loss: 169.15386962890625\n",
            "batch 840 loss: 169.094970703125\n",
            "batch 850 loss: 169.04205322265625\n",
            "batch 860 loss: 168.9778594970703\n",
            "batch 870 loss: 168.93679809570312\n",
            "batch 880 loss: 168.85003662109375\n",
            "batch 890 loss: 168.78631591796875\n",
            "batch 900 loss: 168.71673583984375\n",
            "batch 910 loss: 168.6556396484375\n",
            "batch 920 loss: 168.597412109375\n",
            "batch 930 loss: 168.53123474121094\n",
            "batch 940 loss: 168.46925354003906\n",
            "batch 950 loss: 168.40904235839844\n",
            "batch 960 loss: 168.34097290039062\n",
            "batch 970 loss: 168.2899627685547\n",
            "batch 980 loss: 168.2237548828125\n",
            "batch 990 loss: 168.16221618652344\n",
            "Using scale consts: [1.5625, 1.5625, 21.25, 21.25, 1.5625, 1.5625, 1.5625, 2.6875, 21.25]\n",
            "batch 0 loss: 167.616943359375\n",
            "batch 10 loss: 167.5499267578125\n",
            "batch 20 loss: 167.4553985595703\n",
            "batch 30 loss: 167.36941528320312\n",
            "batch 40 loss: 167.24777221679688\n",
            "batch 50 loss: 167.1809539794922\n",
            "batch 60 loss: 167.07504272460938\n",
            "batch 70 loss: 166.96859741210938\n",
            "batch 80 loss: 166.8811798095703\n",
            "batch 90 loss: 166.7893829345703\n",
            "batch 100 loss: 166.70907592773438\n",
            "batch 110 loss: 166.63092041015625\n",
            "batch 120 loss: 166.5458526611328\n",
            "batch 130 loss: 166.45526123046875\n",
            "batch 140 loss: 166.3726043701172\n",
            "batch 150 loss: 166.30223083496094\n",
            "batch 160 loss: 166.219970703125\n",
            "batch 170 loss: 166.1246795654297\n",
            "batch 180 loss: 166.05152893066406\n",
            "batch 190 loss: 165.96722412109375\n",
            "batch 200 loss: 165.9168701171875\n",
            "batch 210 loss: 165.83766174316406\n",
            "batch 220 loss: 165.74465942382812\n",
            "batch 230 loss: 165.673583984375\n",
            "batch 240 loss: 165.59957885742188\n",
            "batch 250 loss: 165.52764892578125\n",
            "batch 260 loss: 165.45584106445312\n",
            "batch 270 loss: 165.379150390625\n",
            "batch 280 loss: 165.3104705810547\n",
            "batch 290 loss: 165.2484893798828\n",
            "batch 300 loss: 165.1612548828125\n",
            "batch 310 loss: 165.0911865234375\n",
            "batch 320 loss: 165.02011108398438\n",
            "batch 330 loss: 164.9517822265625\n",
            "batch 340 loss: 164.88172912597656\n",
            "batch 350 loss: 164.82298278808594\n",
            "batch 360 loss: 164.74932861328125\n",
            "batch 370 loss: 164.67355346679688\n",
            "batch 380 loss: 164.60931396484375\n",
            "batch 390 loss: 164.5388946533203\n",
            "batch 400 loss: 164.46759033203125\n",
            "batch 410 loss: 164.40744018554688\n",
            "batch 420 loss: 164.34986877441406\n",
            "batch 430 loss: 164.28492736816406\n",
            "batch 440 loss: 164.21737670898438\n",
            "batch 450 loss: 164.16677856445312\n",
            "batch 460 loss: 164.09286499023438\n",
            "batch 470 loss: 164.02584838867188\n",
            "batch 480 loss: 163.95932006835938\n",
            "batch 490 loss: 163.8980255126953\n",
            "batch 500 loss: 163.84426879882812\n",
            "batch 510 loss: 163.7747802734375\n",
            "batch 520 loss: 163.71725463867188\n",
            "batch 530 loss: 163.66943359375\n",
            "batch 540 loss: 163.6061248779297\n",
            "batch 550 loss: 163.55410766601562\n",
            "batch 560 loss: 163.5023193359375\n",
            "batch 570 loss: 163.44415283203125\n",
            "batch 580 loss: 163.38523864746094\n",
            "batch 590 loss: 163.31639099121094\n",
            "batch 600 loss: 163.25369262695312\n",
            "batch 610 loss: 163.18936157226562\n",
            "batch 620 loss: 163.14039611816406\n",
            "batch 630 loss: 163.1053466796875\n",
            "batch 640 loss: 163.03065490722656\n",
            "batch 650 loss: 162.983642578125\n",
            "batch 660 loss: 162.91722106933594\n",
            "batch 670 loss: 162.8842315673828\n",
            "batch 680 loss: 162.83261108398438\n",
            "batch 690 loss: 162.75296020507812\n",
            "batch 700 loss: 162.71414184570312\n",
            "batch 710 loss: 162.63890075683594\n",
            "batch 720 loss: 162.58738708496094\n",
            "batch 730 loss: 162.53955078125\n",
            "batch 740 loss: 162.49180603027344\n",
            "batch 750 loss: 162.43951416015625\n",
            "batch 760 loss: 162.39248657226562\n",
            "batch 770 loss: 162.3351287841797\n",
            "batch 780 loss: 162.26907348632812\n",
            "batch 790 loss: 162.21481323242188\n",
            "batch 800 loss: 162.17015075683594\n",
            "batch 810 loss: 162.11203002929688\n",
            "batch 820 loss: 162.05496215820312\n",
            "batch 830 loss: 162.02142333984375\n",
            "batch 840 loss: 161.9490509033203\n",
            "batch 850 loss: 161.90109252929688\n",
            "batch 860 loss: 161.85748291015625\n",
            "batch 870 loss: 161.8045654296875\n",
            "batch 880 loss: 161.7725830078125\n",
            "batch 890 loss: 161.708740234375\n",
            "batch 900 loss: 161.65147399902344\n",
            "batch 910 loss: 161.59530639648438\n",
            "batch 920 loss: 161.5338897705078\n",
            "batch 930 loss: 161.50323486328125\n",
            "batch 940 loss: 161.4366455078125\n",
            "batch 950 loss: 161.3741455078125\n",
            "batch 960 loss: 161.33575439453125\n",
            "batch 970 loss: 161.28298950195312\n",
            "batch 980 loss: 161.2080535888672\n",
            "batch 990 loss: 161.17929077148438\n",
            "Using scale consts: [2.125, 1.5625, 21.25, 21.25, 2.125, 2.125, 1.5625, 2.6875, 21.25]\n",
            "batch 0 loss: 164.13299560546875\n",
            "batch 10 loss: 164.0661163330078\n",
            "batch 20 loss: 164.02163696289062\n",
            "batch 30 loss: 163.9385528564453\n",
            "batch 40 loss: 163.87937927246094\n",
            "batch 50 loss: 163.7999267578125\n",
            "batch 60 loss: 163.76560974121094\n",
            "batch 70 loss: 163.66905212402344\n",
            "batch 80 loss: 163.60671997070312\n",
            "batch 90 loss: 163.54046630859375\n",
            "batch 100 loss: 163.4757843017578\n",
            "batch 110 loss: 163.43162536621094\n",
            "batch 120 loss: 163.37322998046875\n",
            "batch 130 loss: 163.29763793945312\n",
            "batch 140 loss: 163.2552490234375\n",
            "batch 150 loss: 163.2001190185547\n",
            "batch 160 loss: 163.11642456054688\n",
            "batch 170 loss: 163.03500366210938\n",
            "batch 180 loss: 162.98458862304688\n",
            "batch 190 loss: 162.9550018310547\n",
            "batch 200 loss: 162.8883819580078\n",
            "batch 210 loss: 162.83277893066406\n",
            "batch 220 loss: 162.77186584472656\n",
            "batch 230 loss: 162.7406005859375\n",
            "batch 240 loss: 162.6710968017578\n",
            "batch 250 loss: 162.5997314453125\n",
            "batch 260 loss: 162.55784606933594\n",
            "batch 270 loss: 162.51187133789062\n",
            "batch 280 loss: 162.4556121826172\n",
            "batch 290 loss: 162.3977508544922\n",
            "batch 300 loss: 162.34262084960938\n",
            "batch 310 loss: 162.30838012695312\n",
            "batch 320 loss: 162.24827575683594\n",
            "batch 330 loss: 162.26246643066406\n",
            "batch 340 loss: 162.20059204101562\n",
            "batch 350 loss: 162.1557159423828\n",
            "batch 360 loss: 162.09548950195312\n",
            "batch 370 loss: 162.06387329101562\n",
            "batch 380 loss: 162.0208740234375\n",
            "batch 390 loss: 161.9732208251953\n",
            "batch 400 loss: 161.9288787841797\n",
            "batch 410 loss: 161.8904266357422\n",
            "batch 420 loss: 161.849609375\n",
            "batch 430 loss: 161.8168487548828\n",
            "batch 440 loss: 161.81077575683594\n",
            "batch 450 loss: 161.7480010986328\n",
            "batch 460 loss: 161.70245361328125\n",
            "batch 470 loss: 161.701904296875\n",
            "batch 480 loss: 161.63182067871094\n",
            "batch 490 loss: 161.59483337402344\n",
            "batch 500 loss: 161.55401611328125\n",
            "batch 510 loss: 161.51580810546875\n",
            "batch 520 loss: 161.48406982421875\n",
            "batch 530 loss: 161.43157958984375\n",
            "batch 540 loss: 161.39605712890625\n",
            "batch 550 loss: 161.35032653808594\n",
            "batch 560 loss: 161.31710815429688\n",
            "batch 570 loss: 161.2782440185547\n",
            "batch 580 loss: 161.2431640625\n",
            "batch 590 loss: 161.22264099121094\n",
            "batch 600 loss: 161.1802520751953\n",
            "batch 610 loss: 161.1151123046875\n",
            "batch 620 loss: 161.0716552734375\n",
            "batch 630 loss: 161.0202178955078\n",
            "batch 640 loss: 160.98329162597656\n",
            "batch 650 loss: 160.94430541992188\n",
            "batch 660 loss: 160.88238525390625\n",
            "batch 670 loss: 160.85781860351562\n",
            "batch 680 loss: 160.81100463867188\n",
            "batch 690 loss: 160.7580108642578\n",
            "batch 700 loss: 160.73147583007812\n",
            "batch 710 loss: 160.70152282714844\n",
            "batch 720 loss: 160.64422607421875\n",
            "batch 730 loss: 160.6297607421875\n",
            "batch 740 loss: 160.5755615234375\n",
            "batch 750 loss: 160.53976440429688\n",
            "batch 760 loss: 160.47683715820312\n",
            "batch 770 loss: 160.44216918945312\n",
            "batch 780 loss: 160.40365600585938\n",
            "batch 790 loss: 160.37437438964844\n",
            "batch 800 loss: 160.32577514648438\n",
            "batch 810 loss: 160.2969970703125\n",
            "batch 820 loss: 160.2631072998047\n",
            "batch 830 loss: 160.2183837890625\n",
            "batch 840 loss: 160.19015502929688\n",
            "batch 850 loss: 160.17599487304688\n",
            "batch 860 loss: 160.1315460205078\n",
            "batch 870 loss: 160.09825134277344\n",
            "batch 880 loss: 160.05186462402344\n",
            "batch 890 loss: 160.0078582763672\n",
            "batch 900 loss: 159.9867706298828\n",
            "batch 910 loss: 159.94056701660156\n",
            "batch 920 loss: 159.91575622558594\n",
            "batch 930 loss: 159.89566040039062\n",
            "batch 940 loss: 159.8587188720703\n",
            "batch 950 loss: 159.82809448242188\n",
            "batch 960 loss: 159.78883361816406\n",
            "batch 970 loss: 159.7799072265625\n",
            "batch 980 loss: 159.7220458984375\n",
            "batch 990 loss: 159.69973754882812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display results "
      ],
      "metadata": {
        "id": "Q8dQFRHCshDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision as TV\n",
        "inv_normalize = transforms.Normalize(\n",
        "        mean=mean,\n",
        "        std=std\n",
        "      )\n",
        "\n",
        "for i in range(0,adversarial_examples.size()[0]):\n",
        "\n",
        "  \n",
        "  inv1 = inv_normalize(inputs[i])\n",
        "  TV.utils.save_image(inv1.reshape(28,28), join(paths.MNIST_CW_LOGS, \"%d_original.png\" %(i)),)\n",
        "\n",
        "  inv2 =inv_normalize(adversarial_examples[i]) \n",
        "  TV.utils.save_image(adversarial_examples[i].reshape(28,28), join(paths.MNIST_CW_LOGS, \"%d_adversarial.png\" %(i)))\n",
        "\n",
        "  list_ad = [inv1,inv2]\n",
        "  show(list_ad,gray=True)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rcrHB4kdST2O",
        "outputId": "6fedd4e2-ab10-46ef-858b-ca1a96e3a7bf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAACqCAYAAACTZZUqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAKm0lEQVR4nO3dbWjN/x/H8c/ZRZtMc8NlMVNacrVcZTFRVqzcWK6v7cY0IkqULEoRWS5i2oRawoaSG8SNJblKksjVchUypNXGyMI553/nX//+vd9fnev3OcfzcfP1e5+dD529+v58vp/v8QWDQQcASLwM6wUAwL+KAgYAIxQwABihgAHACAUMAEYoYAAwkhXOsM/n4541xFUwGPQl+j35XCPevD7XYRWwc85lZmZGvxpA4ff7zd47I4P/GUR8BAIBz//Gpw4AjFDAAGCEAgYAIxQwABihgAHACAUMAEYoYAAwQgEDgBEKGACMUMAAYIQCBgAjFDAAGKGAAcAIBQwARihgADBCAQOAEQoYAIxQwABghAIGACMUMAAYoYABwAgFDABGKGAAMEIBA4CRLOsFJItly5aJrLu7W51tbm4W2YcPH0T2/fv3kF/f2tqqzpaVlam55uLFiyK7evVqyK9H+snJyRHZ2rVr1dna2lqRffz4UWQLFy5UXz9y5EiRff78WZ0tKioSmfZ74ZxzM2bMENnJkyfV2VTDFTAAGKGAAcAIBQwARihgADBCAQOAEV8wGAx92OcLZmZmxnE5dgYMGCCyV69eRfUzKyoq1HzLli0iKykpieq9nHNu3bp1ItPu7vBaw/3796NeQzT8fr8LBoO+RL+vz+cLZmSk57XIo0ePRLZ161Z19s2bNyJ7/PixyLzu7tHuuMjOzlZnf//+LbKnT5+qszdv3hRZdXW1OjthwgSRPXv2TJ1NlEAg4Pm5Ts9PHQCkAAoYAIxQwABghAIGACP/3Cbc27dv1Tw3N1dkeXl5Ub3Xnz9/1HzevHkiu337tjpbX18vMq8j0l4bbprr16+LbPbs2SG/Ph7YhIvcxo0b1Xzv3r0JW8Po0aNF9uDBA3V2/vz5Ivvy5Ys6e+fOHZF5/W717dtXZF1dXepsorAJBwBJiAIGACMUMAAYoYABwAgFDABG0vouiEuXLonM63hxVVVVVO+l3ZngdWSzsLAwqvdqbGxUc+39Kisr1VntjomWlpaQf248cBdEaHr16iUyn0//aysvLxfZvn371FntzoJ+/fqJ7OvXr+rrBw4cqOah0o4cO+fcpEmTROb3+9XZhoYGka1fv16dTVSXcRcEACQhChgAjFDAAGCEAgYAI2m9Cbd8+XKRaUd7Y2Hbtm0iO3DgQFzey4u2wag959iL1+Zge3t7pEsKC5twoampqRHZqlWr1NlBgwZF9V4jRowQ2YsXL6L6meHSnlNcUFAQ8uu9njNcXFwc8ZrCwSYcACQhChgAjFDAAGCEAgYAIxQwABjJsl5ALHg9ZD1eu/fag8uj3W2Ohbq6OpHt3Lkz5Nd7za5evTriNSFy2kPLnXOuR48eIvv06ZM6qz3kfNy4ceqsdhR9ypQpIkv0XRD79+8XWW1trTqrHcnWjm4nC66AAcAIBQwARihgADBCAQOAkbQ4iuz1HNKXL19G9XO9nh3c2dkpsunTp0f1XrEwatQokWVl6fust27dEtnDhw/V2dLS0ugWFiKOIv+/u3fvqrl2hNbrW4Kzs7NF5vXsYG2z6tevX39bYkIsXrxYZF6/29rsvXv31NmzZ89Gt7AQcRQZAJIQBQwARihgADBCAQOAEQoYAIykxVHklStXhjyrHeN0zrljx46J7P79++rs8ePHQ36/RHry5InILl++bLASxILXt/neuHFDZG1tbers7t27Reb1+U2GOx40TU1NIvO6u0f7Fudov605nrgCBgAjFDAAGKGAAcAIBQwARtJiEy4c06ZNU/OKigqR/WvPwU2GZxrjf5YuXRr1z9CO5mrHk1PN+fPn1fzixYsimzNnjjp74sSJmK4pElwBA4ARChgAjFDAAGCEAgYAIym3CVdWViaycDaPNm3apOYDBgyIeE3JrKOjI+RZ7Us9kRja84hXrVqlzmqbaIsWLVJnvZ7xnOpqamrU/NChQyKrr6+P93IixhUwABihgAHACAUMAEYoYAAwQgEDgJGUuwuivb1dZJWVlYlfSIqoqqpSc+3oNezk5+eLrLy8XJ19/fq1yM6dO6fOTp48ObqFJakLFy6o+YQJE0Q2fvz4eC8nYlwBA4ARChgAjFDAAGCEAgYAIym3Cac9o/f69evq7PTp0+O7mDQzdOhQ6yX8s/78+SOyK1euqLOZmZki2759e8zXlMy0L9F1zrlAICCyU6dOqbODBw+O6ZoiwRUwABihgAHACAUMAEYoYAAwQgEDgJG0uAtiy5Yt6ix3QXgfRdYUFxfHcSX4m66uLpGNGTNGndU+7y9evIj5mpLZt2/f1PzTp08i27x5c7yXEzGugAHACAUMAEYoYAAwQgEDgJGU24TTeH0jrCY3N1fNtaOgWVmp/9fj9UxZJD+vo+ElJSUi27NnjzpbUFAgsvfv30e3sCQwZcoUNe/Zs6fIHjx4EO/lRIwrYAAwQgEDgBEKGACMUMAAYIQCBgAjqb/N75zr3bt3yLOlpaVqng53PDQ0NIgsmb8RFn9XU1Oj5kVFRSLbtWuXOrtgwQKR+Xy+6BaWYNqfbcmSJSG//syZM2qu/T0mGlfAAGCEAgYAIxQwABihgAHASOrvPDnnhg8fruatra0i69Onjzr7+fNnkR08eFCd9Tr2ae3Hjx8iy8vLU2e1DTuvWdjIyclR8+fPn4ts2LBh6mwwGBSZ1wZWU1NTGKtLnI6ODpFlZOjXjmvWrBHZu3fvYr6mWOEKGACMUMAAYIQCBgAjFDAAGKGAAcCIT9sl9Rz2+YKZmZlxXE5srVu3TmSFhYXqrPZty2/fvlVnOzs7ReZ1xDkeuru7Q87DOWJtfReE3+93wWAw4edkfT5f0GtXPRn9/PlTZIcPH1ZnN27cKLKWlhZ1dsWKFSL78uVLmKuLnPZ75Vx4n0vtLievn5sogUDA83OdOp86AEgzFDAAGKGAAcAIBQwARtLiKLKXuro6kXltrH3//l1kXht2lZWVIWVex0M1R44cUfNJkyaJTPsGZ+f0TTivDbtw1obk0qNHD5HdvHlTnZ04caLItCP6zjk3ZMgQkc2cOVNkXkeWBw0aJDLt98I55+bOnSsy7c/lnL5Brn37sXPOff36Vc2TFVfAAGCEAgYAIxQwABihgAHACAUMAEbS+ihyOEaNGiWyu3fvGqwkcqdPnxZZdXW1wUoiw1Hk2Js6darItLuDnHPu6NGjIuvXr5/Iurq61NePHTtWZFevXlVnGxsb1VzT3NwssmXLloX8emscRQaAJEQBA4ARChgAjFDAAGCETbj/ys3NFZnXsWVtU6CqqirWS3LOOdfW1iay/Px8dVY7Ou11FDkZsQkXe9euXRPZjh071FntiLH2zeD9+/dXX79hw4aQ16V9s3hxcbE6O2vWLJFlZ2eH/F7W2IQDgCREAQOAEQoYAIxQwABghAIGACPcBYGkwV0QSEfcBQEASYgCBgAjFDAAGKGAAcAIBQwARihgADBCAQOAEQoYAIxQwABghAIGACMUMAAYoYABwAgFDABGKGAAMEIBA4CRrDDn2/1+/7u4rARwbojR+7YHAgE+14gXz891WA9kBwDEDv8EAQBGKGAAMEIBA4ARChgAjFDAAGCEAgYAIxQwABihgAHACAUMAEb+A7mRvQYlErlkAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}